{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iTXhOLj8hfJN"
   },
   "source": [
    "https://github.com/jiechuanjiang/pytorch_DGN/blob/main/Surviving/DGN%2BATOC/config.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(14)\n",
    "import math, random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd \n",
    "import torch.nn.functional as F\n",
    "import copy\n",
    "import os,sys\n",
    "from torch.utils import tensorboard as tb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "eJJIdzHAhTGs"
   },
   "outputs": [],
   "source": [
    "hidden_dim = 64\n",
    "max_step = 5000 #originally 500\n",
    "GAMMA = 0.99\n",
    "n_episode = 1000 #originally 800\n",
    "i_episode = 0\n",
    "buffer_size = 65000 #change back to 65000\n",
    "batch_size = 64 #change back to 64\n",
    "n_epoch = 100 #orginally 25\n",
    "epsilon = 0.7 #originally 0.9\n",
    "score = 0\n",
    "tau = 0.98\n",
    "\n",
    "GRID_DIM = 50 # TODO: Tune this\n",
    "NUM_TASKS = 2 # TODO: Tune this\n",
    "ADJ_THRESHOLD = GRID_DIM / 4 # TODO: Tune this\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "Variable = lambda *args, **kwargs: autograd.Variable(*args, **kwargs).cuda() if USE_CUDA else autograd.Variable(*args, **kwargs)\n",
    "\n",
    "def is_legal(x,y):\n",
    "    return (x>=0)&(x<GRID_DIM)&(y>=0)&(y<=GRID_DIM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IUxf35c1hpGD"
   },
   "source": [
    "https://github.com/jiechuanjiang/pytorch_DGN/blob/main/Surviving/DGN%2BATOC/buffer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBufferGCare(object):\n",
    "    \"\"\"\n",
    "    Replay buffer for storing the agent's experiences\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, buffer_size, obs_space, n_action, n_tasks):\n",
    "        \"\"\"\n",
    "        Initialize the replay buffer\n",
    "        \n",
    "        Params:\n",
    "        buffer_size:\n",
    "        obs_space:\n",
    "        n_action:\n",
    "        n_tasks:\n",
    "        \"\"\"\n",
    "        self.buffer_size = buffer_size\n",
    "        self.n_tasks = n_tasks\n",
    "        self.pointer = 0\n",
    "        self.len = 0\n",
    "        self.actions = np.zeros((self.buffer_size,1),dtype = np.int32)\n",
    "        self.rewards = np.zeros((self.buffer_size, 1))\n",
    "        self.dones = np.zeros((self.buffer_size,1))\n",
    "        self.obs = np.zeros((self.buffer_size,n_tasks,obs_space))\n",
    "        self.next_obs = np.zeros((self.buffer_size,n_tasks,obs_space))\n",
    "        self.matrix = np.zeros((self.buffer_size,self.n_tasks,self.n_tasks))\n",
    "        self.next_matrix = np.zeros((self.buffer_size,self.n_tasks,self.n_tasks))\n",
    "\n",
    "    def getBatch(self, batch_size):\n",
    "        \"\"\"\n",
    "        Sample a batch of random entries from the replay buffer\n",
    "        \n",
    "        Params:\n",
    "        batch_size:\n",
    "        \n",
    "        Returns:\n",
    "        obs:\n",
    "        action:\n",
    "        reward\n",
    "        next_obs:\n",
    "        matrix:\n",
    "        next_matrix:\n",
    "        done:\n",
    "        \"\"\"\n",
    "        index = np.random.choice(self.len, batch_size, replace=False)\n",
    "        return self.obs[index], self.actions[index], self.rewards[index], self.next_obs[index], self.matrix[index], self.next_matrix[index], self.dones[index]\n",
    "\n",
    "    def add(self, obs, action, reward, next_obs, matrix, next_matrix, done):\n",
    "        \"\"\"\n",
    "        Add to the replay buffer\n",
    "        \n",
    "        Params:\n",
    "        obs:\n",
    "        action:\n",
    "        reward:\n",
    "        next_obs:\n",
    "        matrix:\n",
    "        next_matrix:\n",
    "        done:\n",
    "        \"\"\"\n",
    "        self.obs[self.pointer] = obs\n",
    "        self.actions[self.pointer] = action\n",
    "        self.rewards[self.pointer] = reward\n",
    "        self.next_obs[self.pointer] = next_obs\n",
    "        self.matrix[self.pointer] = matrix\n",
    "        self.next_matrix[self.pointer] = next_matrix\n",
    "        self.dones[self.pointer] = done\n",
    "        self.pointer = (self.pointer + 1)%self.buffer_size\n",
    "        self.len = min(self.len + 1, self.buffer_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wONM8xyshxjL"
   },
   "source": [
    "https://github.com/jiechuanjiang/pytorch_DGN/blob/main/Surviving/DGN%2BATOC/model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MTRL_ATT(nn.Module):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    def __init__(self, din):\n",
    "        super(MTRL_ATT, self).__init__()\n",
    "        self.fc1 = nn.Linear(din, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = F.relu(self.fc1(x))\n",
    "        y = F.relu(self.fc2(y))\n",
    "        y = F.sigmoid(self.fc3(y))\n",
    "        return y\n",
    "\n",
    "class MTRL_Encoder(nn.Module): # TODO: Need to make it a CNN for higher dim obs space like MetaWorld\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    def __init__(self, din=32, hidden_dim=128):\n",
    "        super(MTRL_Encoder, self).__init__()\n",
    "        self.fc = nn.Linear(din, hidden_dim)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedding = F.tanh(self.fc(x))\n",
    "        return embedding\n",
    "\n",
    "class MTRL_AttModel(nn.Module):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    def __init__(self, n_node, din, hidden_dim, dout):\n",
    "        super(MTRL_AttModel, self).__init__()\n",
    "        self.fcv = nn.Linear(din, hidden_dim)\n",
    "        self.fck = nn.Linear(din, hidden_dim)\n",
    "        self.fcq = nn.Linear(din, hidden_dim)\n",
    "        self.fcout = nn.Linear(hidden_dim, dout)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        v = F.tanh(self.fcv(x))\n",
    "        q = F.tanh(self.fcq(x))\n",
    "        k = F.tanh(self.fck(x)).permute(0,2,1)\n",
    "        att = F.softmax(torch.mul(torch.bmm(q,k), mask) - 9e15*(1 - mask),dim=2)\n",
    "        # Note: Order of applying adj matrix is different than that in paper. Don't get confused!\n",
    "        out = torch.bmm(att,v)\n",
    "        return out\n",
    "\n",
    "class MTRL_Q_Net(nn.Module):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_dim, dout):\n",
    "        super(MTRL_Q_Net, self).__init__()\n",
    "        # NOTE: This is now modified to have both h vectors from both of the attention layers\n",
    "        # concatenated - originally it was only getting the h vector of the last layer\n",
    "        # so the input dim of the linear layer was hidden_dim\n",
    "        self.fc = nn.Linear(hidden_dim*2, dout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        q = F.relu(self.fc(x))\n",
    "        return q\n",
    "\n",
    "    \n",
    "class MTRL_DGN(nn.Module):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    def __init__(self,n_tasks,num_inputs,hidden_dim,num_actions):\n",
    "        super(MTRL_DGN, self).__init__()\n",
    "\n",
    "        self.encoder = MTRL_Encoder(num_inputs,hidden_dim)\n",
    "        # TODO: Try both single encoder and mix of encoder settings\n",
    "        # Will remain same for MTRL\n",
    "        self.att_1 = MTRL_AttModel(n_tasks,hidden_dim,hidden_dim,hidden_dim)\n",
    "        self.att_2 = MTRL_AttModel(n_tasks,hidden_dim,hidden_dim,hidden_dim)\n",
    "        self.q_net = MTRL_Q_Net(hidden_dim,num_actions)\n",
    "        # Q Net remains same for MTRL\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        h1 = self.encoder(x)\n",
    "        h2 = self.att_1(h1, mask)\n",
    "        h3 = self.att_2(h2, mask) \n",
    "        # TODO: try concatentation for MTRL\n",
    "        \n",
    "        h4 = torch.cat((h2,h3),dim=2)\n",
    "        q = self.q_net(h4)\n",
    "        # Note: No concatenation done. Output of last attention head used directly\n",
    "        # Note: 2 attention heads used\n",
    "        return q "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MxG8-x_piggE"
   },
   "source": [
    "https://github.com/jiechuanjiang/pytorch_DGN/blob/main/Surviving/DGN%2BATOC/surviving.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorldWithCare(object):\n",
    "    \n",
    "    def __init__(self, n_tasks):\n",
    "        \"\"\"\n",
    "        Initialize the gridworld\n",
    "        \n",
    "        Params:\n",
    "        n_tasks:\n",
    "        \"\"\"\n",
    "        super(GridWorldWithCare, self).__init__()\n",
    "        self.n_action = 4\n",
    "        self.n_tasks = n_tasks\n",
    "        # TODO: maybe include food as part of task, reach dest with > 0 food or something\n",
    "        self.tasks = [0]*self.n_tasks\n",
    "        self.agent = [-1, -1]\n",
    "        self.build_env()\n",
    "\n",
    "        self.dones = np.zeros(self.n_tasks) # Array to indicate whether each task is done or not -- used to calculate rewards\n",
    "        self.steps = 0\n",
    "        self.len_obs = (self.n_tasks+1)*2\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Reset the gridworld\n",
    "        \n",
    "        Returns:\n",
    "        obs:\n",
    "        adj:\n",
    "        \"\"\"\n",
    "\n",
    "        self.build_env()\n",
    "        self.dones = np.zeros(self.n_tasks)\n",
    "        self.steps = 0\n",
    "        return self.get_obs(), self.get_adj()\n",
    "\n",
    "    def build_env(self):\n",
    "        \"\"\"\n",
    "        Build the gridworld\n",
    "        \"\"\"\n",
    "        for i in range(self.n_tasks):\n",
    "            x = np.random.randint(0, GRID_DIM)\n",
    "            y = np.random.randint(0, GRID_DIM)\n",
    "            self.tasks[i] = [x, y]\n",
    "            print(\"TASK NUMBER \", i, \" DEST: \", x, y)\n",
    "        self.agent[0] = np.random.randint(0, GRID_DIM)\n",
    "        self.agent[1] = np.random.randint(0, GRID_DIM)\n",
    "\n",
    "    def get_obs(self):\n",
    "        \"\"\"\n",
    "        Get observations\n",
    "        \n",
    "        Returns:\n",
    "        obs:\n",
    "        \"\"\"\n",
    "        # TODO: change this for MTRL \n",
    "        obs = []\n",
    "        \n",
    "        x_agent = self.agent[0]\n",
    "        y_agent = self.agent[1]\n",
    "\n",
    "        obs.append(x_agent/GRID_DIM)\n",
    "        obs.append(y_agent/GRID_DIM)\n",
    "\n",
    "        # \t\tfor i in range(-1,2):\n",
    "        # \t\t\tfor j in range(-1,2):\n",
    "        # \t\t\t\tobs.append(self.maze[x_agent+i][y_agent+j])\n",
    "\n",
    "        for i in range(self.n_tasks):\n",
    "            obs.append((self.tasks[i][0]-x_agent)/GRID_DIM)\n",
    "            obs.append((self.tasks[i][1]-y_agent)/GRID_DIM)\n",
    "\n",
    "        # TODO: 1. if we include maze state or not, and if we do, we would need to figure out\n",
    "        # how to effectively send that along with task destinations\n",
    "        \n",
    "        #Idea: use distance between agent and task as obs\n",
    "        \n",
    "        return obs\n",
    "\n",
    "    def get_adj(self): # TODO: Change this to use task description encoding. \n",
    "        # In this case task description is the location of the destination.\n",
    "        \"\"\"\n",
    "        Get adjacency matrix\n",
    "        \n",
    "        Returns:\n",
    "        adj:\n",
    "        \"\"\"\n",
    "        adj = np.zeros((self.n_tasks, self.n_tasks))\n",
    "\n",
    "        # Calculate adjacency regarding to the distances of the tasks respect to the agent\n",
    "        x_agent, y_agent = self.agent[0], self.agent[1]\n",
    "\n",
    "        # HARD ATTENTION\n",
    "        # Traverse through the tasks and calculate the Euclidean distance between them and the agent\n",
    "#         for i in range(self.n_tasks):\n",
    "#             x_task_i, y_task_i = self.tasks[i][0] - x_agent, self.tasks[i][1] - y_agent\n",
    "#             for j in range(self.n_tasks):\n",
    "#                 x_task_j, y_task_j = self.tasks[j][0] - x_agent, self.tasks[j][1] - y_agent\n",
    "#                 task_dist = math.sqrt((x_task_j - x_task_i)**2 + (y_task_i - y_task_j)**2)\n",
    "#                 if task_dist <= ADJ_THRESHOLD:\n",
    "#                     adj[i,j] = 1\n",
    "#                     adj[j,i] = 1\n",
    "                    \n",
    "        # SOFT ATTENTION\n",
    "#         adj = np.ones((self.n_tasks, self.n_tasks)) # NOTE: \n",
    "        for i in range(self.n_tasks):\n",
    "            x_task_i, y_task_i = self.tasks[i][0]-x_agent, self.tasks[i][1]-y_agent\n",
    "            for j in range(self.n_tasks):\n",
    "                x_task_j, y_task_j = self.tasks[j][0]-x_agent, self.tasks[j][1]-y_agent\n",
    "                # Instead of having 1 or 0s, have their vectoral positions according to each other\n",
    "                task_dist = math.sqrt((x_task_j - x_task_i)**2 + (y_task_j - y_task_i)**2)\n",
    "                \n",
    "#                 print('x_task_i: {}, y_task_i: {}, x_task_j: {}, y_task_j: {}, task_dist: {}'.format(\n",
    "#                         x_task_i, y_task_i, x_task_j, y_task_j, task_dist\n",
    "#                 ))\n",
    "                \n",
    "                # Set this distance / GRID_DIM\n",
    "                adj[i,j] = 1 - float(task_dist)/GRID_DIM # Extract from 1 bc the closer the better\n",
    "                adj[j,i] = 1 - float(task_dist)/GRID_DIM\n",
    "                \n",
    "        \n",
    "                \n",
    "#         print(\"ADJACENCY: {}\".format(adj))\n",
    "\n",
    "#         print('x_agent: {}, y_agent: {}'.format(x_agent, y_agent))\n",
    "\n",
    "        return adj\n",
    "\n",
    "\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Take one step in the gridworld according to the given actions\n",
    "        \n",
    "        Params:\n",
    "        action:\n",
    "        \n",
    "        Returns:\n",
    "        obs:\n",
    "        adj:\n",
    "        reward:\n",
    "        all_tasks_done:\n",
    "        \"\"\"\n",
    "\n",
    "        # There are 4 different actions for the agent\n",
    "        # If there is any place to go in the maze then the agent will go \n",
    "        # 0: Move up, 1: Move down, 2: Move left, 3: Move right\n",
    "\n",
    "        self.steps += 1\n",
    "        x_agent, y_agent = self.agent[0], self.agent[1]\n",
    "#         print(\"AGENT LOCATION: \", agent_x, agent_y)\n",
    "#         print(\"ACTION: \", action)\n",
    "        if action == 0: # Move up (decrease x by one)\n",
    "            if is_legal(x_agent-1, y_agent):\n",
    "                # Change the agent and the maze\n",
    "                self.agent[0] -= 1\n",
    "\n",
    "        elif action == 1: # Move down (increase x by one)\n",
    "            if is_legal(x_agent+1, y_agent):\n",
    "                # Change the agent and the maze\n",
    "                self.agent[0] += 1\n",
    "\n",
    "        elif action == 2: # Move left (decrease y by one)\n",
    "            if is_legal(x_agent, y_agent-1):\n",
    "                # Change the agent and the maze\n",
    "                self.agent[1] -= 1\n",
    "\n",
    "        elif action == 3: # Move right (increase y by one)\n",
    "            if is_legal(x_agent, y_agent+1):\n",
    "                # Change the agent and the maze\n",
    "                self.agent[1] += 1\n",
    "                \n",
    "        # Calculate the rewards for each task\n",
    "        rewards = [0] * self.n_tasks\n",
    "        total_reward = 0\n",
    "\n",
    "        # Check if you reached to any destinations here\n",
    "        new_agent_x, new_agent_y = self.agent[0], self.agent[1]\n",
    "        for i in range(self.n_tasks):\n",
    "            if self.tasks[i][0] == new_agent_x and self.tasks[i][1] == new_agent_y:\n",
    "                if self.dones[i] == 0:\n",
    "                    self.dones[i] = 1\n",
    "                    rewards[i] = 1\n",
    "                    total_reward += 1\n",
    "                    print(\"Task \", i, \" completed at step \", self.steps)\n",
    "            else:\n",
    "                total_reward += 1.0/float((math.sqrt((self.tasks[i][0]-new_agent_x)**2 + (self.tasks[i][1]-new_agent_y)**2)))\n",
    "                \n",
    "\n",
    "        # Only if all the tasks are done, then the episode is done\n",
    "        all_tasks_done = not (0 in self.dones)\n",
    "\n",
    "\n",
    "\n",
    "        return self.get_obs(), self.get_adj(), total_reward, all_tasks_done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0D64gsJRimGp"
   },
   "source": [
    "https://github.com/jiechuanjiang/pytorch_DGN/blob/main/Surviving/DGN%2BATOC/main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 400
    },
    "id": "gNfVZBScitk1",
    "outputId": "1c74fa00-341a-479c-e219-183896503acb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TASK NUMBER  0  DEST:  23 6\n",
      "TASK NUMBER  1  DEST:  34 33\n",
      "TASK NUMBER  0  DEST:  31 25\n",
      "TASK NUMBER  1  DEST:  4 46\n",
      "TASK NUMBER  0  DEST:  21 28\n",
      "TASK NUMBER  1  DEST:  1 19\n",
      "Task  1  completed at step  220\n",
      "TASK NUMBER  0  DEST:  27 39\n",
      "TASK NUMBER  1  DEST:  3 39\n",
      "TASK NUMBER  0  DEST:  8 44\n",
      "TASK NUMBER  1  DEST:  0 34\n",
      "TASK NUMBER  0  DEST:  28 42\n",
      "TASK NUMBER  1  DEST:  15 49\n",
      "TASK NUMBER  0  DEST:  21 21\n",
      "TASK NUMBER  1  DEST:  32 46\n",
      "Task  0  completed at step  44\n",
      "TASK NUMBER  0  DEST:  17 8\n",
      "TASK NUMBER  1  DEST:  5 14\n",
      "TASK NUMBER  0  DEST:  26 38\n",
      "TASK NUMBER  1  DEST:  14 33\n",
      "TASK NUMBER  0  DEST:  20 9\n",
      "TASK NUMBER  1  DEST:  46 3\n",
      "TASK NUMBER  0  DEST:  11 45\n",
      "TASK NUMBER  1  DEST:  19 19\n",
      "TASK NUMBER  0  DEST:  47 45\n",
      "TASK NUMBER  1  DEST:  23 18\n",
      "TASK NUMBER  0  DEST:  42 32\n",
      "TASK NUMBER  1  DEST:  6 36\n",
      "TASK NUMBER  0  DEST:  1 16\n",
      "TASK NUMBER  1  DEST:  17 23\n",
      "TASK NUMBER  0  DEST:  32 15\n",
      "TASK NUMBER  1  DEST:  32 12\n",
      "TASK NUMBER  0  DEST:  35 49\n",
      "TASK NUMBER  1  DEST:  15 2\n",
      "Task  1  completed at step  3436\n",
      "TASK NUMBER  0  DEST:  9 30\n",
      "TASK NUMBER  1  DEST:  11 31\n",
      "TASK NUMBER  0  DEST:  8 24\n",
      "TASK NUMBER  1  DEST:  45 30\n",
      "Task  0  completed at step  21\n",
      "TASK NUMBER  0  DEST:  20 43\n",
      "TASK NUMBER  1  DEST:  0 13\n"
     ]
    }
   ],
   "source": [
    "env = GridWorldWithCare(NUM_TASKS)\n",
    "observation_space = env.len_obs\n",
    "n_actions = env.n_action\n",
    "n_tasks = env.n_tasks\n",
    "\n",
    "buff = ReplayBufferGCare(buffer_size,observation_space,n_actions,n_tasks)\n",
    "model = MTRL_DGN(n_tasks,observation_space,hidden_dim,n_actions)\n",
    "model_tar = MTRL_DGN(n_tasks,observation_space,hidden_dim,n_actions)\n",
    "model = model.cuda()\n",
    "model_tar = model_tar.cuda()\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.001)\n",
    "# att = MTRL_ATT(observation_space).cuda()\n",
    "# att_tar = MTRL_ATT(observation_space).cuda()\n",
    "# att_tar.load_state_dict(att.state_dict())\n",
    "# optimizer_att = optim.Adam(att.parameters(), lr = 0.001)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "M_Null = torch.Tensor(np.array([np.eye(n_tasks)]*batch_size)).cuda()\n",
    "M_ZERO = torch.Tensor(np.zeros((batch_size,n_tasks,n_tasks))).cuda()\n",
    "# threshold = float(sys.argv[1]) TODO: figure this out\n",
    "# f = open(sys.argv[1]+'-'+sys.argv[2]+'.txt','w+')\n",
    "log_file_name = \"TRIAL-9-SoftAttention-Concat-WithTB\"\n",
    "f = open(log_file_name+\".txt\", \"w+\")\n",
    "while i_episode<n_episode:\n",
    "    if i_episode > 40:\n",
    "        epsilon -= 0.001\n",
    "        if epsilon < 0.01:\n",
    "            epsilon = 0.01\n",
    "    i_episode+=1\n",
    "    steps = 0\n",
    "    obs, adj = env.reset()\n",
    "    obs = np.resize(obs, (n_tasks, observation_space))\n",
    "    episode_summary_writer = tb.SummaryWriter(log_dir='./TB-Logs/'+log_file_name)\n",
    "    episode_epoch_count = 0\n",
    "    while steps < max_step:\n",
    "        steps+=1 \n",
    "#         cost_all += adj.sum()\n",
    "#         v_a = np.array(att(torch.Tensor(np.array([obs])).cuda())[0].cpu().data)\n",
    "#         for i in range(n_tasks):\n",
    "#             if np.random.rand() < epsilon:\n",
    "#                 adj[i] = adj[i]*0 if np.random.rand() < 0.5 else adj[i]*1\n",
    "#             else:\n",
    "#                 adj[i] = adj[i]*0 if v_a[i][0] < threshold else adj[i]*1\n",
    "        # Note: above loop is epsilon greedy exploration to give less importance to observations that fall below a certain threshold\n",
    "        # May not be needed if we use single encoder but could be useful in the case of mixture of encoders\n",
    "        # Pruning \"less imp\" neighbours whose obs fall below a certain threshold\n",
    "#         n_adj = adj*comm_flag\n",
    "#         cost_comm += n_adj.sum()\n",
    "#         n_adj = n_adj + np.eye(n_tasks)\n",
    "#         q_dummy = model(torch.Tensor(np.array([obs])).cuda(), torch.Tensor(np.array([adj])).cuda())\n",
    "#         print(\"model output shape\", q_dummy.shape)\n",
    "        q = model(torch.Tensor(np.array([obs])).cuda(), torch.Tensor(np.array([adj])).cuda())[0,0,:]\n",
    "#         print(\"Shape of Q: \", q.shape)\n",
    "        if np.random.rand() < epsilon:\n",
    "#             print(\"HERE RANDOM\")\n",
    "            a = np.random.randint(n_actions)\n",
    "        else:\n",
    "#             print(\"HERE FROM MODEL\")\n",
    "            a = q.argmax().item()\n",
    "\n",
    "        action = a\n",
    "        \n",
    "        next_obs, next_adj, reward, terminated = env.step(action)\n",
    "#         print('action: {}, next_obs: {}\\nnext_adj:\\n{}'.format(\n",
    "#             action, next_obs, next_adj\n",
    "#         ))\n",
    "        \n",
    "        next_obs = np.resize(next_obs, (n_tasks, observation_space))\n",
    "        \n",
    "        buff.add(np.array(obs),action,reward,np.array(next_obs),adj,next_adj,terminated)\n",
    "        \n",
    "        obs = next_obs\n",
    "        adj = next_adj\n",
    "        score += reward\n",
    "\n",
    "    if i_episode%20==0:\n",
    "        print(score)\n",
    "        #print(score/2000)\n",
    "        f.write(str(score)+'\\n')\n",
    "        episode_summary_writer.add_scalar(\"Score/Episode\", score, i_episode)\n",
    "        # Cost (neighbors in adj matrix)after pruning/ Cost before pruning\n",
    "        f.flush()\n",
    "        score = 0\n",
    "\n",
    "#     if i_episode < 40:\n",
    "#         continue\n",
    "\n",
    "    for e in range(n_epoch):\n",
    "\n",
    "        episode_epoch_count += 1\n",
    "        O,A,R,Next_O,Matrix,Next_Matrix,D = buff.getBatch(batch_size)\n",
    "        O = torch.Tensor(O).cuda()\n",
    "        Matrix = torch.Tensor(Matrix).cuda()\n",
    "        Next_O = torch.Tensor(Next_O).cuda()\n",
    "        Next_Matrix = torch.Tensor(Next_Matrix).cuda()\n",
    "\n",
    "#         label = model(Next_O, Next_Matrix+M_Null).max(dim = 2)[0] - model(Next_O, M_Null).max(dim = 2)[0]\n",
    "#         #print(\"Label\", label.shape)\n",
    "#         label = (label - label.mean())/(label.std()+0.000001) + 0.5\n",
    "#         label = torch.clamp(label, 0, 1).unsqueeze(-1).detach()\n",
    "#         #print(\"Label after clamping\", label.shape)\n",
    "#         #print(\"ATT output\", label_dummy.shape)\n",
    "#         loss = criterion(a(Next_O), label)\n",
    "#         optimizer_att.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer_att.step()\n",
    "        # Basically att is learning which obs from the maze help return the max q value\n",
    "\n",
    "#         V_A_D = att_tar(Next_O).expand(-1,-1,n_ant)\n",
    "#         Next_Matrix = torch.where(V_A_D > threshold, Next_Matrix, M_ZERO)\n",
    "#         Next_Matrix = Next_Matrix*comm_flag + M_Null\n",
    "\n",
    "        q_values = model(O, Matrix)\n",
    "#         print(\"Q Vals Before slicing: \", q_values.shape)\n",
    "        q_values = model(O, Matrix)[:,0, :]\n",
    "#         print(\"Q Vals After slicing: \", q_values_final.shape)\n",
    "        target_q_values = model_tar(Next_O, Next_Matrix).max(dim = 2)[0][:,0]\n",
    "#         print(\"Target Q Vals: \", target_q_values.shape)\n",
    "        target_q_values = np.array(target_q_values.cpu().data)\n",
    "        expected_q = np.array(q_values.cpu().data)\n",
    "\n",
    "        for j in range(batch_size):\n",
    "#             for i in range(n_tasks):\n",
    "            expected_q[j][A[j][0]] = R[j][0] + (1-D[j][0])*GAMMA*target_q_values[j]\n",
    "\n",
    "        loss = (q_values - torch.Tensor(expected_q).cuda()).pow(2).mean()\n",
    "        episode_summary_writer.add_scalar('Loss', loss, episode_epoch_count)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if e%10 == 0:\n",
    "            with torch.no_grad():\n",
    "                for p, p_targ in zip(model.parameters(), model_tar.parameters()):\n",
    "                    p_targ.data.mul_(tau)\n",
    "                    p_targ.data.add_((1 - tau) * p.data)\n",
    "#                 for p, p_targ in zip(att.parameters(), att_tar.parameters()):\n",
    "#                     p_targ.data.mul_(tau)\n",
    "#                     p_targ.data.add_((1 - tau) * p.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "DRL-FinalProject-GridWorld.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "my_pytorch_env",
   "language": "python",
   "name": "my_pytorch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
