{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iTXhOLj8hfJN"
   },
   "source": [
    "https://github.com/jiechuanjiang/pytorch_DGN/blob/main/Surviving/DGN%2BATOC/config.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(14)\n",
    "import math, random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd \n",
    "import torch.nn.functional as F\n",
    "import copy\n",
    "import os,sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "eJJIdzHAhTGs"
   },
   "outputs": [],
   "source": [
    "hidden_dim = 64\n",
    "max_step = 5000 #originally 500\n",
    "GAMMA = 0.99\n",
    "n_episode = 1000 #originally 800\n",
    "i_episode = 0\n",
    "buffer_size = 65000 #change back to 65000\n",
    "batch_size = 64 #change back to 64\n",
    "n_epoch = 100 #orginally 25\n",
    "epsilon = 0.7 #originally 0.9\n",
    "score = 0\n",
    "tau = 0.98\n",
    "\n",
    "GRID_DIM = 50 # TODO: Tune this\n",
    "NUM_TASKS = 2 # TODO: Tune this\n",
    "ADJ_THRESHOLD = GRID_DIM / 4 # TODO: Tune this\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "Variable = lambda *args, **kwargs: autograd.Variable(*args, **kwargs).cuda() if USE_CUDA else autograd.Variable(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IUxf35c1hpGD"
   },
   "source": [
    "https://github.com/jiechuanjiang/pytorch_DGN/blob/main/Surviving/DGN%2BATOC/buffer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBufferGCare(object):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, buffer_size, obs_space, n_action, n_tasks):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        self.buffer_size = buffer_size\n",
    "        self.n_tasks = n_tasks\n",
    "        self.pointer = 0\n",
    "        self.len = 0\n",
    "        self.actions = np.zeros((self.buffer_size,1),dtype = np.int32)\n",
    "        self.rewards = np.zeros((self.buffer_size, 1))\n",
    "        self.dones = np.zeros((self.buffer_size,1))\n",
    "        self.obs = np.zeros((self.buffer_size,n_tasks,obs_space))\n",
    "        self.next_obs = np.zeros((self.buffer_size,n_tasks,obs_space))\n",
    "        self.matrix = np.zeros((self.buffer_size,self.n_tasks,self.n_tasks))\n",
    "        self.next_matrix = np.zeros((self.buffer_size,self.n_tasks,self.n_tasks))\n",
    "\n",
    "    def getBatch(self, batch_size):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        index = np.random.choice(self.len, batch_size, replace=False)\n",
    "        return self.obs[index], self.actions[index], self.rewards[index], self.next_obs[index], self.matrix[index], self.next_matrix[index], self.dones[index]\n",
    "\n",
    "    def add(self, obs, action, reward, next_obs, matrix, next_matrix, done):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        self.obs[self.pointer] = obs\n",
    "        self.actions[self.pointer] = action\n",
    "        self.rewards[self.pointer] = reward\n",
    "        self.next_obs[self.pointer] = next_obs\n",
    "        self.matrix[self.pointer] = matrix\n",
    "        self.next_matrix[self.pointer] = next_matrix\n",
    "        self.dones[self.pointer] = done\n",
    "        self.pointer = (self.pointer + 1)%self.buffer_size\n",
    "        self.len = min(self.len + 1, self.buffer_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wONM8xyshxjL"
   },
   "source": [
    "https://github.com/jiechuanjiang/pytorch_DGN/blob/main/Surviving/DGN%2BATOC/model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MTRL_ATT(nn.Module):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    def __init__(self, din):\n",
    "        super(MTRL_ATT, self).__init__()\n",
    "        self.fc1 = nn.Linear(din, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = F.relu(self.fc1(x))\n",
    "        y = F.relu(self.fc2(y))\n",
    "        y = F.sigmoid(self.fc3(y))\n",
    "        return y\n",
    "\n",
    "class MTRL_Encoder(nn.Module): # TODO: Need to make it a CNN for higher dim obs space like MetaWorld\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    def __init__(self, din=32, hidden_dim=128):\n",
    "        super(MTRL_Encoder, self).__init__()\n",
    "        self.fc = nn.Linear(din, hidden_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedding = F.tanh(self.fc(x))\n",
    "        return embedding\n",
    "\n",
    "class MTRL_AttModel(nn.Module):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    def __init__(self, n_node, din, hidden_dim, dout):\n",
    "        super(MTRL_AttModel, self).__init__()\n",
    "        self.fcv = nn.Linear(din, hidden_dim)\n",
    "        self.fck = nn.Linear(din, hidden_dim)\n",
    "        self.fcq = nn.Linear(din, hidden_dim)\n",
    "        self.fcout = nn.Linear(hidden_dim, dout)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "#         print(\"Att model input shape: \", x.shape)\n",
    "        v = F.tanh(self.fcv(x))\n",
    "        q = F.tanh(self.fcq(x))\n",
    "#         print(\"Att model q layer shape: \", q.shape)\n",
    "        k = F.tanh(self.fck(x)).permute(0,2,1)\n",
    "        att = F.softmax(torch.mul(torch.bmm(q,k), mask) - 9e15*(1 - mask),dim=2)\n",
    "        # Note: Order of applying adj matrix is different than that in paper. Don't get confused!\n",
    "        out = torch.bmm(att,v)\n",
    "        #out = torch.add(out,v)\n",
    "        #out = F.relu(self.fcout(out))\n",
    "        return out\n",
    "\n",
    "class MTRL_Q_Net(nn.Module):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_dim, dout):\n",
    "        super(MTRL_Q_Net, self).__init__()\n",
    "        self.fc = nn.Linear(hidden_dim, dout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        q = F.relu(self.fc(x))\n",
    "        return q\n",
    "\n",
    "    \n",
    "class MTRL_DGN(nn.Module):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    def __init__(self,n_tasks,num_inputs,hidden_dim,num_actions):\n",
    "        super(MTRL_DGN, self).__init__()\n",
    "\n",
    "        self.encoder = MTRL_Encoder(num_inputs,hidden_dim)\n",
    "        # TODO: Try both single encoder and mix of encoder settings\n",
    "        # Will remain same for MTRL\n",
    "        self.att_1 = MTRL_AttModel(n_tasks,hidden_dim,hidden_dim,hidden_dim)\n",
    "        self.att_2 = MTRL_AttModel(n_tasks,hidden_dim,hidden_dim,hidden_dim)\n",
    "        self.q_net = MTRL_Q_Net(hidden_dim,num_actions)\n",
    "        # Q Net remains same for MTRL\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        h1 = self.encoder(x)\n",
    "        h2 = self.att_1(h1, mask)\n",
    "        h3 = self.att_2(h2, mask)\n",
    "        # TODO: try concatentation for MTRL\n",
    "        q = self.q_net(h3)\n",
    "        # Note: No concatenation done. Output of last attention head used directly\n",
    "        # Note: 2 attention heads used\n",
    "        return q "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MxG8-x_piggE"
   },
   "source": [
    "https://github.com/jiechuanjiang/pytorch_DGN/blob/main/Surviving/DGN%2BATOC/surviving.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_legal(x,y):\n",
    "    return (x>=0)&(x<GRID_DIM)&(y>=0)&(y<=GRID_DIM)\n",
    "\n",
    "class GridWorldWithCare(object):\n",
    "    \n",
    "    def __init__(self, n_tasks):\n",
    "        \"\"\"\n",
    "        Initialize the gridworld\n",
    "        \n",
    "        params:\n",
    "        \n",
    "        returns:\n",
    "        \"\"\"\n",
    "        super(GridWorldWithCare, self).__init__()\n",
    "        self.n_action = 4\n",
    "        self.n_tasks = n_tasks\n",
    "        # TODO: maybe include food as part of task, reach dest with > 0 food or something\n",
    "        self.tasks = [0]*self.n_tasks\n",
    "        self.agent = [-1, -1]\n",
    "        self.build_env()\n",
    "\n",
    "        self.dones = np.zeros(self.n_tasks) # Array to indicate whether each task is done or not -- used to calculate rewards\n",
    "        self.steps = 0\n",
    "        self.len_obs = (self.n_tasks+1)*2\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Reset the gridworld\n",
    "        \n",
    "        returns:\n",
    "        \"\"\"\n",
    "\n",
    "        self.build_env()\n",
    "        self.dones = np.zeros(self.n_tasks)\n",
    "        self.steps = 0\n",
    "        return self.get_obs(), self.get_adj()\n",
    "\n",
    "    def build_env(self):\n",
    "        \"\"\"\n",
    "        Build the gridworld\n",
    "        \n",
    "        returns:\n",
    "        \"\"\"\n",
    "        for i in range(self.n_tasks):\n",
    "            x = np.random.randint(0, GRID_DIM)\n",
    "            y = np.random.randint(0, GRID_DIM)\n",
    "            self.tasks[i] = [x, y]\n",
    "            print(\"TASK NUMBER \", i, \" DEST: \", x, y)\n",
    "        self.agent[0] = np.random.randint(0, GRID_DIM)\n",
    "        self.agent[1] = np.random.randint(0, GRID_DIM)\n",
    "\n",
    "    def get_obs(self):\n",
    "        \"\"\"\n",
    "        Get observations\n",
    "        \n",
    "        returns:\n",
    "        \"\"\"\n",
    "        # TODO: change this for MTRL \n",
    "        obs = []\n",
    "        \n",
    "        x_agent = self.agent[0]\n",
    "        y_agent = self.agent[1]\n",
    "\n",
    "        obs.append(x_agent/GRID_DIM)\n",
    "        obs.append(y_agent/GRID_DIM)\n",
    "\n",
    "        # \t\tfor i in range(-1,2):\n",
    "        # \t\t\tfor j in range(-1,2):\n",
    "        # \t\t\t\tobs.append(self.maze[x_agent+i][y_agent+j])\n",
    "\n",
    "        for i in range(self.n_tasks):\n",
    "            obs.append((self.tasks[i][0]-x_agent)/GRID_DIM)\n",
    "            obs.append((self.tasks[i][1]-y_agent)/GRID_DIM)\n",
    "\n",
    "        # TODO: 1. if we include maze state or not, and if we do, we would need to figure out\n",
    "        # how to effectively send that along with task destinations\n",
    "        \n",
    "        #Idea: use distance between agent and task as obs\n",
    "        \n",
    "        return obs\n",
    "\n",
    "    def get_adj(self): # TODO: Change this to use task description encoding. \n",
    "        # In this case task description is the location of the destination.\n",
    "        \"\"\"\n",
    "        Get adjacency matrix\n",
    "        \n",
    "        returns:\n",
    "        \n",
    "        \"\"\"\n",
    "        adj = np.zeros((self.n_tasks, self.n_tasks))\n",
    "\n",
    "        # Calculate adjacency regarding to the distances of the tasks respect to the agent\n",
    "        x_agent, y_agent = self.agent[0], self.agent[1]\n",
    "#         task_distances = np.zeros((self.n_tasks, self.n_tasks))\n",
    "\n",
    "        # Traverse through the tasks and calculate the Euclidean distance between them and the agent\n",
    "        for i in range(self.n_tasks):\n",
    "            x_task_i, y_task_i = self.tasks[i][0] - x_agent, self.tasks[i][1] - y_agent\n",
    "#             task_i_dist = math.sqrt((x_task - x_agent)**2 + (y_task - y_agent)**2)\n",
    "#             print(\"Task \", i, \"Distance: \", task_i_dist)\n",
    "            for j in range(self.n_tasks):\n",
    "                x_task_j, y_task_j = self.tasks[j][0] - x_agent, self.tasks[j][1] - y_agent\n",
    "                task_dist = math.sqrt((x_task_j - x_task_i)**2 + (y_task_i - y_task_j)**2)\n",
    "#                 print(\"Task \", j, \"Distance: \", task_j_dist)\n",
    "                if task_dist <= ADJ_THRESHOLD:\n",
    "                    adj[i,j] = 1\n",
    "                    adj[j,i] = 1\n",
    "        \n",
    "#         for i in range(self.n_tasks):\n",
    "#             for j in range(self.n_tasks):   \n",
    "#                 adj[i][j] = \n",
    "        \n",
    "#         print(adj)\n",
    "\n",
    "        return adj\n",
    "\n",
    "\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Take one step in the gridworld according to the given actions\n",
    "        \n",
    "        params:\n",
    "        self - \n",
    "        action - \n",
    "        \n",
    "        returns:\n",
    "        obs - \n",
    "        adj - \n",
    "        reward - \n",
    "        all_tasks_done - \n",
    "        \"\"\"\n",
    "\n",
    "        # There are 4 different actions for the agent\n",
    "        # If there is any place to go in the maze then the agent will go \n",
    "        # 0: Move up, 1: Move down, 2: Move left, 3: Move right\n",
    "\n",
    "        self.steps += 1\n",
    "        agent_x, agent_y = self.agent[0], self.agent[1]\n",
    "#         print(\"AGENT LOCATION: \", agent_x, agent_y)\n",
    "#         print(\"ACTION: \", action)\n",
    "        if action == 0: # Move up (decrease x by one)\n",
    "            if is_legal(agent_x-1, agent_y):\n",
    "                # Change the agent and the maze\n",
    "                self.agent[0] -= 1\n",
    "\n",
    "        elif action == 1: # Move down (increase x by one)\n",
    "            if is_legal(agent_x+1, agent_y):\n",
    "                # Change the agent and the maze\n",
    "                self.agent[0] += 1\n",
    "\n",
    "        elif action == 2: # Move left (decrease y by one)\n",
    "            if is_legal(agent_x, agent_y-1):\n",
    "                # Change the agent and the maze\n",
    "                self.agent[1] -= 1\n",
    "\n",
    "        elif action == 3: # Move right (increase y by one)\n",
    "            if is_legal(agent_x, agent_y+1):\n",
    "                # Change the agent and the maze\n",
    "                self.agent[1] += 1\n",
    "                \n",
    "        # Calculate the rewards for each task\n",
    "        rewards = [0] * self.n_tasks\n",
    "        total_reward = 0\n",
    "\n",
    "        # Check if you reached to any destinations here\n",
    "        new_agent_x, new_agent_y = self.agent[0], self.agent[1]\n",
    "        for i in range(self.n_tasks):\n",
    "            if self.tasks[i][0] == new_agent_x and self.tasks[i][1] == new_agent_y:\n",
    "                if self.dones[i] == 0:\n",
    "                    self.dones[i] = 1\n",
    "                    rewards[i] = 1\n",
    "                    total_reward += 1\n",
    "                    print(\"Task \", i, \" completed at step \", self.steps)\n",
    "            else:\n",
    "                total_reward += 1.0/float((math.sqrt((self.tasks[i][0]-new_agent_x)**2 + (self.tasks[i][1]-new_agent_y)**2)))\n",
    "                \n",
    "\n",
    "        # Only if all the tasks are done, then the episode is done\n",
    "        all_tasks_done = not (0 in self.dones)\n",
    "\n",
    "\n",
    "\n",
    "        return self.get_obs(), self.get_adj(), total_reward, all_tasks_done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0D64gsJRimGp"
   },
   "source": [
    "https://github.com/jiechuanjiang/pytorch_DGN/blob/main/Surviving/DGN%2BATOC/main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 400
    },
    "id": "gNfVZBScitk1",
    "outputId": "1c74fa00-341a-479c-e219-183896503acb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TASK NUMBER  0  DEST:  43 24\n",
      "TASK NUMBER  1  DEST:  12 22\n",
      "TASK NUMBER  0  DEST:  42 28\n",
      "TASK NUMBER  1  DEST:  10 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ext3/miniconda3/lib/python3.9/site-packages/torch/nn/functional.py:1698: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TASK NUMBER  0  DEST:  17 38\n",
      "TASK NUMBER  1  DEST:  44 33\n",
      "Task  1  completed at step  358\n",
      "TASK NUMBER  0  DEST:  3 25\n",
      "TASK NUMBER  1  DEST:  46 49\n",
      "Task  0  completed at step  1048\n",
      "TASK NUMBER  0  DEST:  24 43\n",
      "TASK NUMBER  1  DEST:  12 45\n",
      "TASK NUMBER  0  DEST:  41 16\n",
      "TASK NUMBER  1  DEST:  49 38\n",
      "Task  0  completed at step  3545\n",
      "TASK NUMBER  0  DEST:  11 15\n",
      "TASK NUMBER  1  DEST:  1 15\n",
      "TASK NUMBER  0  DEST:  21 36\n",
      "TASK NUMBER  1  DEST:  6 32\n",
      "TASK NUMBER  0  DEST:  16 33\n",
      "TASK NUMBER  1  DEST:  5 12\n",
      "Task  0  completed at step  581\n",
      "TASK NUMBER  0  DEST:  44 17\n",
      "TASK NUMBER  1  DEST:  31 26\n",
      "Task  0  completed at step  2086\n",
      "TASK NUMBER  0  DEST:  43 32\n",
      "TASK NUMBER  1  DEST:  30 10\n",
      "TASK NUMBER  0  DEST:  16 48\n",
      "TASK NUMBER  1  DEST:  19 6\n",
      "TASK NUMBER  0  DEST:  20 20\n",
      "TASK NUMBER  1  DEST:  5 46\n",
      "TASK NUMBER  0  DEST:  48 33\n",
      "TASK NUMBER  1  DEST:  7 13\n",
      "TASK NUMBER  0  DEST:  21 37\n",
      "TASK NUMBER  1  DEST:  0 2\n",
      "TASK NUMBER  0  DEST:  15 39\n",
      "TASK NUMBER  1  DEST:  11 16\n",
      "TASK NUMBER  0  DEST:  1 1\n",
      "TASK NUMBER  1  DEST:  0 14\n",
      "Task  1  completed at step  1344\n",
      "Task  0  completed at step  1409\n",
      "TASK NUMBER  0  DEST:  25 44\n",
      "TASK NUMBER  1  DEST:  27 45\n",
      "TASK NUMBER  0  DEST:  4 36\n",
      "TASK NUMBER  1  DEST:  5 39\n",
      "TASK NUMBER  0  DEST:  46 7\n",
      "TASK NUMBER  1  DEST:  12 25\n",
      "TASK NUMBER  0  DEST:  17 21\n",
      "TASK NUMBER  1  DEST:  18 46\n",
      "11608.356742730188\n",
      "TASK NUMBER  0  DEST:  27 28\n",
      "TASK NUMBER  1  DEST:  38 12\n",
      "TASK NUMBER  0  DEST:  20 45\n",
      "TASK NUMBER  1  DEST:  36 49\n",
      "Task  1  completed at step  210\n",
      "TASK NUMBER  0  DEST:  20 34\n",
      "TASK NUMBER  1  DEST:  47 7\n",
      "TASK NUMBER  0  DEST:  44 37\n",
      "TASK NUMBER  1  DEST:  20 29\n",
      "TASK NUMBER  0  DEST:  10 1\n",
      "TASK NUMBER  1  DEST:  48 34\n",
      "TASK NUMBER  0  DEST:  34 41\n",
      "TASK NUMBER  1  DEST:  8 48\n",
      "TASK NUMBER  0  DEST:  35 5\n",
      "TASK NUMBER  1  DEST:  47 1\n",
      "TASK NUMBER  0  DEST:  42 9\n",
      "TASK NUMBER  1  DEST:  43 40\n",
      "Task  0  completed at step  69\n",
      "TASK NUMBER  0  DEST:  36 28\n",
      "TASK NUMBER  1  DEST:  37 21\n",
      "Task  0  completed at step  1115\n",
      "Task  1  completed at step  1801\n",
      "TASK NUMBER  0  DEST:  44 31\n",
      "TASK NUMBER  1  DEST:  14 4\n",
      "TASK NUMBER  0  DEST:  30 41\n",
      "TASK NUMBER  1  DEST:  18 37\n",
      "TASK NUMBER  0  DEST:  36 38\n",
      "TASK NUMBER  1  DEST:  45 22\n",
      "TASK NUMBER  0  DEST:  33 22\n",
      "TASK NUMBER  1  DEST:  45 40\n",
      "Task  0  completed at step  407\n",
      "TASK NUMBER  0  DEST:  17 48\n",
      "TASK NUMBER  1  DEST:  24 30\n",
      "TASK NUMBER  0  DEST:  0 0\n",
      "TASK NUMBER  1  DEST:  42 32\n",
      "Task  0  completed at step  3181\n",
      "TASK NUMBER  0  DEST:  33 46\n",
      "TASK NUMBER  1  DEST:  43 2\n",
      "TASK NUMBER  0  DEST:  22 14\n",
      "TASK NUMBER  1  DEST:  35 1\n",
      "TASK NUMBER  0  DEST:  46 7\n",
      "TASK NUMBER  1  DEST:  15 21\n",
      "TASK NUMBER  0  DEST:  1 14\n",
      "TASK NUMBER  1  DEST:  34 31\n",
      "TASK NUMBER  0  DEST:  31 29\n",
      "TASK NUMBER  1  DEST:  35 38\n",
      "13824.511424911994\n",
      "TASK NUMBER  0  DEST:  20 28\n",
      "TASK NUMBER  1  DEST:  18 14\n",
      "TASK NUMBER  0  DEST:  31 49\n",
      "TASK NUMBER  1  DEST:  12 42\n",
      "TASK NUMBER  0  DEST:  3 36\n",
      "TASK NUMBER  1  DEST:  12 38\n",
      "Task  1  completed at step  2048\n",
      "Task  0  completed at step  3211\n",
      "TASK NUMBER  0  DEST:  9 19\n",
      "TASK NUMBER  1  DEST:  21 6\n",
      "TASK NUMBER  0  DEST:  7 6\n",
      "TASK NUMBER  1  DEST:  32 34\n",
      "TASK NUMBER  0  DEST:  15 3\n",
      "TASK NUMBER  1  DEST:  44 19\n",
      "TASK NUMBER  0  DEST:  8 9\n",
      "TASK NUMBER  1  DEST:  25 34\n",
      "TASK NUMBER  0  DEST:  13 43\n",
      "TASK NUMBER  1  DEST:  39 12\n",
      "TASK NUMBER  0  DEST:  7 6\n",
      "TASK NUMBER  1  DEST:  44 10\n",
      "TASK NUMBER  0  DEST:  40 4\n",
      "TASK NUMBER  1  DEST:  4 20\n",
      "TASK NUMBER  0  DEST:  23 0\n",
      "TASK NUMBER  1  DEST:  22 18\n",
      "Task  0  completed at step  436\n",
      "TASK NUMBER  0  DEST:  42 12\n",
      "TASK NUMBER  1  DEST:  16 27\n",
      "Task  1  completed at step  37\n",
      "Task  0  completed at step  3966\n",
      "TASK NUMBER  0  DEST:  34 47\n",
      "TASK NUMBER  1  DEST:  7 25\n",
      "TASK NUMBER  0  DEST:  48 24\n",
      "TASK NUMBER  1  DEST:  32 10\n",
      "TASK NUMBER  0  DEST:  21 19\n",
      "TASK NUMBER  1  DEST:  22 48\n",
      "TASK NUMBER  0  DEST:  46 33\n",
      "TASK NUMBER  1  DEST:  35 18\n",
      "Task  1  completed at step  1247\n",
      "TASK NUMBER  0  DEST:  2 30\n",
      "TASK NUMBER  1  DEST:  25 33\n",
      "TASK NUMBER  0  DEST:  4 3\n",
      "TASK NUMBER  1  DEST:  6 31\n",
      "Task  1  completed at step  4474\n",
      "TASK NUMBER  0  DEST:  3 4\n",
      "TASK NUMBER  1  DEST:  44 48\n",
      "TASK NUMBER  0  DEST:  37 41\n",
      "TASK NUMBER  1  DEST:  16 13\n",
      "15307.564158738407\n",
      "TASK NUMBER  0  DEST:  14 10\n",
      "TASK NUMBER  1  DEST:  21 48\n",
      "TASK NUMBER  0  DEST:  28 3\n",
      "TASK NUMBER  1  DEST:  34 43\n",
      "Task  0  completed at step  142\n",
      "TASK NUMBER  0  DEST:  45 7\n",
      "TASK NUMBER  1  DEST:  15 30\n",
      "Task  0  completed at step  38\n",
      "TASK NUMBER  0  DEST:  8 3\n",
      "TASK NUMBER  1  DEST:  27 9\n",
      "TASK NUMBER  0  DEST:  24 32\n",
      "TASK NUMBER  1  DEST:  44 1\n",
      "TASK NUMBER  0  DEST:  0 7\n",
      "TASK NUMBER  1  DEST:  39 1\n",
      "TASK NUMBER  0  DEST:  34 46\n",
      "TASK NUMBER  1  DEST:  18 40\n",
      "TASK NUMBER  0  DEST:  11 49\n",
      "TASK NUMBER  1  DEST:  24 20\n",
      "TASK NUMBER  0  DEST:  48 16\n",
      "TASK NUMBER  1  DEST:  40 19\n",
      "TASK NUMBER  0  DEST:  33 5\n",
      "TASK NUMBER  1  DEST:  37 47\n",
      "TASK NUMBER  0  DEST:  9 21\n",
      "TASK NUMBER  1  DEST:  48 20\n",
      "TASK NUMBER  0  DEST:  0 41\n",
      "TASK NUMBER  1  DEST:  20 6\n",
      "TASK NUMBER  0  DEST:  33 35\n",
      "TASK NUMBER  1  DEST:  0 14\n",
      "TASK NUMBER  0  DEST:  27 15\n",
      "TASK NUMBER  1  DEST:  20 33\n",
      "TASK NUMBER  0  DEST:  32 1\n",
      "TASK NUMBER  1  DEST:  41 33\n",
      "Task  1  completed at step  561\n",
      "Task  0  completed at step  3668\n",
      "TASK NUMBER  0  DEST:  23 21\n",
      "TASK NUMBER  1  DEST:  49 27\n",
      "TASK NUMBER  0  DEST:  14 7\n",
      "TASK NUMBER  1  DEST:  40 21\n",
      "TASK NUMBER  0  DEST:  26 22\n",
      "TASK NUMBER  1  DEST:  4 24\n",
      "Task  0  completed at step  1194\n",
      "TASK NUMBER  0  DEST:  25 28\n",
      "TASK NUMBER  1  DEST:  38 16\n",
      "Task  1  completed at step  147\n",
      "TASK NUMBER  0  DEST:  11 16\n",
      "TASK NUMBER  1  DEST:  7 29\n",
      "13541.402591301168\n",
      "TASK NUMBER  0  DEST:  46 21\n",
      "TASK NUMBER  1  DEST:  17 44\n",
      "TASK NUMBER  0  DEST:  21 33\n",
      "TASK NUMBER  1  DEST:  40 41\n",
      "TASK NUMBER  0  DEST:  29 33\n",
      "TASK NUMBER  1  DEST:  5 44\n",
      "TASK NUMBER  0  DEST:  10 5\n",
      "TASK NUMBER  1  DEST:  6 19\n",
      "TASK NUMBER  0  DEST:  2 33\n",
      "TASK NUMBER  1  DEST:  17 5\n",
      "TASK NUMBER  0  DEST:  40 1\n",
      "TASK NUMBER  1  DEST:  41 1\n",
      "TASK NUMBER  0  DEST:  11 35\n",
      "TASK NUMBER  1  DEST:  6 49\n",
      "TASK NUMBER  0  DEST:  42 28\n",
      "TASK NUMBER  1  DEST:  48 14\n",
      "Task  1  completed at step  2458\n",
      "TASK NUMBER  0  DEST:  40 25\n",
      "TASK NUMBER  1  DEST:  23 12\n",
      "TASK NUMBER  0  DEST:  8 11\n",
      "TASK NUMBER  1  DEST:  26 35\n",
      "TASK NUMBER  0  DEST:  44 24\n",
      "TASK NUMBER  1  DEST:  11 12\n",
      "TASK NUMBER  0  DEST:  26 47\n",
      "TASK NUMBER  1  DEST:  46 39\n",
      "TASK NUMBER  0  DEST:  7 0\n",
      "TASK NUMBER  1  DEST:  15 41\n",
      "TASK NUMBER  0  DEST:  3 47\n",
      "TASK NUMBER  1  DEST:  46 23\n",
      "TASK NUMBER  0  DEST:  4 11\n",
      "TASK NUMBER  1  DEST:  18 6\n",
      "Task  1  completed at step  384\n",
      "TASK NUMBER  0  DEST:  37 13\n",
      "TASK NUMBER  1  DEST:  49 47\n",
      "TASK NUMBER  0  DEST:  14 36\n",
      "TASK NUMBER  1  DEST:  37 30\n",
      "TASK NUMBER  0  DEST:  4 7\n",
      "TASK NUMBER  1  DEST:  47 49\n",
      "TASK NUMBER  0  DEST:  6 18\n",
      "TASK NUMBER  1  DEST:  23 7\n",
      "Task  1  completed at step  66\n",
      "TASK NUMBER  0  DEST:  21 43\n",
      "TASK NUMBER  1  DEST:  7 23\n",
      "Task  1  completed at step  879\n",
      "16311.482764114635\n",
      "TASK NUMBER  0  DEST:  11 33\n",
      "TASK NUMBER  1  DEST:  22 6\n",
      "TASK NUMBER  0  DEST:  27 45\n",
      "TASK NUMBER  1  DEST:  6 41\n",
      "TASK NUMBER  0  DEST:  49 28\n",
      "TASK NUMBER  1  DEST:  35 24\n",
      "Task  0  completed at step  257\n",
      "TASK NUMBER  0  DEST:  19 14\n",
      "TASK NUMBER  1  DEST:  44 49\n",
      "TASK NUMBER  0  DEST:  46 12\n",
      "TASK NUMBER  1  DEST:  29 44\n",
      "Task  0  completed at step  4514\n",
      "TASK NUMBER  0  DEST:  49 2\n",
      "TASK NUMBER  1  DEST:  12 43\n",
      "TASK NUMBER  0  DEST:  13 1\n",
      "TASK NUMBER  1  DEST:  27 3\n",
      "Task  1  completed at step  388\n",
      "TASK NUMBER  0  DEST:  34 21\n",
      "TASK NUMBER  1  DEST:  0 45\n",
      "TASK NUMBER  0  DEST:  1 8\n",
      "TASK NUMBER  1  DEST:  5 49\n",
      "Task  0  completed at step  175\n",
      "TASK NUMBER  0  DEST:  42 9\n",
      "TASK NUMBER  1  DEST:  15 46\n",
      "TASK NUMBER  0  DEST:  0 45\n",
      "TASK NUMBER  1  DEST:  34 27\n",
      "TASK NUMBER  0  DEST:  2 48\n",
      "TASK NUMBER  1  DEST:  9 40\n",
      "Task  0  completed at step  2290\n",
      "TASK NUMBER  0  DEST:  25 21\n",
      "TASK NUMBER  1  DEST:  19 23\n",
      "Task  0  completed at step  89\n",
      "TASK NUMBER  0  DEST:  9 36\n",
      "TASK NUMBER  1  DEST:  4 46\n",
      "Task  1  completed at step  2116\n",
      "TASK NUMBER  0  DEST:  16 23\n",
      "TASK NUMBER  1  DEST:  25 47\n",
      "Task  0  completed at step  1506\n",
      "TASK NUMBER  0  DEST:  38 9\n",
      "TASK NUMBER  1  DEST:  41 23\n",
      "TASK NUMBER  0  DEST:  2 37\n",
      "TASK NUMBER  1  DEST:  32 47\n",
      "Task  0  completed at step  2479\n",
      "TASK NUMBER  0  DEST:  42 29\n",
      "TASK NUMBER  1  DEST:  3 18\n"
     ]
    }
   ],
   "source": [
    "env = GridWorldWithCare(NUM_TASKS)\n",
    "observation_space = env.len_obs\n",
    "n_actions = env.n_action\n",
    "n_tasks = env.n_tasks\n",
    "\n",
    "buff = ReplayBufferGCare(buffer_size,observation_space,n_actions,n_tasks)\n",
    "model = MTRL_DGN(n_tasks,observation_space,hidden_dim,n_actions)\n",
    "model_tar = MTRL_DGN(n_tasks,observation_space,hidden_dim,n_actions)\n",
    "model = model.cuda()\n",
    "model_tar = model_tar.cuda()\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.001)\n",
    "# att = MTRL_ATT(observation_space).cuda()\n",
    "# att_tar = MTRL_ATT(observation_space).cuda()\n",
    "# att_tar.load_state_dict(att.state_dict())\n",
    "# optimizer_att = optim.Adam(att.parameters(), lr = 0.001)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "M_Null = torch.Tensor(np.array([np.eye(n_tasks)]*batch_size)).cuda()\n",
    "M_ZERO = torch.Tensor(np.zeros((batch_size,n_tasks,n_tasks))).cuda()\n",
    "# threshold = float(sys.argv[1]) TODO: figure this out\n",
    "# f = open(sys.argv[1]+'-'+sys.argv[2]+'.txt','w+')\n",
    "f = open(\"TRIAL-9.txt\", \"w+\")\n",
    "while i_episode<n_episode:\n",
    "    if i_episode > 40:\n",
    "        epsilon -= 0.001\n",
    "        if epsilon < 0.01:\n",
    "            epsilon = 0.01\n",
    "    i_episode+=1\n",
    "    steps = 0\n",
    "    obs, adj = env.reset()\n",
    "    obs = np.resize(obs, (n_tasks, observation_space))\n",
    "    while steps < max_step:\n",
    "        steps+=1 \n",
    "#         cost_all += adj.sum()\n",
    "#         v_a = np.array(att(torch.Tensor(np.array([obs])).cuda())[0].cpu().data)\n",
    "#         for i in range(n_tasks):\n",
    "#             if np.random.rand() < epsilon:\n",
    "#                 adj[i] = adj[i]*0 if np.random.rand() < 0.5 else adj[i]*1\n",
    "#             else:\n",
    "#                 adj[i] = adj[i]*0 if v_a[i][0] < threshold else adj[i]*1\n",
    "        # Note: above loop is epsilon greedy exploration to give less importance to observations that fall below a certain threshold\n",
    "        # May not be needed if we use single encoder but could be useful in the case of mixture of encoders\n",
    "        # Pruning \"less imp\" neighbours whose obs fall below a certain threshold\n",
    "#         n_adj = adj*comm_flag\n",
    "#         cost_comm += n_adj.sum()\n",
    "#         n_adj = n_adj + np.eye(n_tasks)\n",
    "#         q_dummy = model(torch.Tensor(np.array([obs])).cuda(), torch.Tensor(np.array([adj])).cuda())\n",
    "#         print(\"model output shape\", q_dummy.shape)\n",
    "        q = model(torch.Tensor(np.array([obs])).cuda(), torch.Tensor(np.array([adj])).cuda())[0,0,:]\n",
    "#         print(\"Shape of Q: \", q.shape)\n",
    "        if np.random.rand() < epsilon:\n",
    "#             print(\"HERE RANDOM\")\n",
    "            a = np.random.randint(n_actions)\n",
    "        else:\n",
    "#             print(\"HERE FROM MODEL\")\n",
    "            a = q.argmax().item()\n",
    "\n",
    "        action = a\n",
    "        \n",
    "        next_obs, next_adj, reward, terminated = env.step(action)\n",
    "        \n",
    "        next_obs = np.resize(next_obs, (n_tasks, observation_space))\n",
    "        \n",
    "        buff.add(np.array(obs),action,reward,np.array(next_obs),adj,next_adj,terminated)\n",
    "        \n",
    "        obs = next_obs\n",
    "        adj = next_adj\n",
    "        score += reward\n",
    "\n",
    "    if i_episode%20==0:\n",
    "        print(score)\n",
    "        #print(score/2000)\n",
    "        f.write(str(score)+'\\n')\n",
    "        # Cost (neighbors in adj matrix)after pruning/ Cost before pruning\n",
    "        f.flush()\n",
    "        score = 0\n",
    "\n",
    "#     if i_episode < 40:\n",
    "#         continue\n",
    "\n",
    "    for e in range(n_epoch):\n",
    "\n",
    "        O,A,R,Next_O,Matrix,Next_Matrix,D = buff.getBatch(batch_size)\n",
    "        O = torch.Tensor(O).cuda()\n",
    "        Matrix = torch.Tensor(Matrix).cuda()\n",
    "        Next_O = torch.Tensor(Next_O).cuda()\n",
    "        Next_Matrix = torch.Tensor(Next_Matrix).cuda()\n",
    "\n",
    "#         label = model(Next_O, Next_Matrix+M_Null).max(dim = 2)[0] - model(Next_O, M_Null).max(dim = 2)[0]\n",
    "#         #print(\"Label\", label.shape)\n",
    "#         label = (label - label.mean())/(label.std()+0.000001) + 0.5\n",
    "#         label = torch.clamp(label, 0, 1).unsqueeze(-1).detach()\n",
    "#         #print(\"Label after clamping\", label.shape)\n",
    "#         #print(\"ATT output\", label_dummy.shape)\n",
    "#         loss = criterion(a(Next_O), label)\n",
    "#         optimizer_att.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer_att.step()\n",
    "        # Basically att is learning which obs from the maze help return the max q value\n",
    "\n",
    "#         V_A_D = att_tar(Next_O).expand(-1,-1,n_ant)\n",
    "#         Next_Matrix = torch.where(V_A_D > threshold, Next_Matrix, M_ZERO)\n",
    "#         Next_Matrix = Next_Matrix*comm_flag + M_Null\n",
    "\n",
    "        q_values = model(O, Matrix)\n",
    "#         print(\"Q Vals Before slicing: \", q_values.shape)\n",
    "        q_values = model(O, Matrix)[:,0, :]\n",
    "#         print(\"Q Vals After slicing: \", q_values_final.shape)\n",
    "        target_q_values = model_tar(Next_O, Next_Matrix).max(dim = 2)[0][:,0]\n",
    "#         print(\"Target Q Vals: \", target_q_values.shape)\n",
    "        target_q_values = np.array(target_q_values.cpu().data)\n",
    "        expected_q = np.array(q_values.cpu().data)\n",
    "\n",
    "        for j in range(batch_size):\n",
    "#             for i in range(n_tasks):\n",
    "            expected_q[j][A[j][0]] = R[j][0] + (1-D[j][0])*GAMMA*target_q_values[j]\n",
    "\n",
    "        loss = (q_values - torch.Tensor(expected_q).cuda()).pow(2).mean()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if e%10 == 0:\n",
    "            with torch.no_grad():\n",
    "                for p, p_targ in zip(model.parameters(), model_tar.parameters()):\n",
    "                    p_targ.data.mul_(tau)\n",
    "                    p_targ.data.add_((1 - tau) * p.data)\n",
    "#                 for p, p_targ in zip(att.parameters(), att_tar.parameters()):\n",
    "#                     p_targ.data.mul_(tau)\n",
    "#                     p_targ.data.add_((1 - tau) * p.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "DRL-FinalProject-GridWorld.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "my_pytorch_env",
   "language": "python",
   "name": "my_pytorch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
