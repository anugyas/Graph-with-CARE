{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iTXhOLj8hfJN"
   },
   "source": [
    "https://github.com/jiechuanjiang/pytorch_DGN/blob/main/Surviving/DGN%2BATOC/config.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "eJJIdzHAhTGs"
   },
   "outputs": [],
   "source": [
    "hidden_dim = 64\n",
    "max_step = 500\n",
    "GAMMA = 0.99\n",
    "n_episode = 800\n",
    "i_episode = 0\n",
    "capacity = 650\n",
    "batch_size = 32\n",
    "n_epoch = 25\n",
    "epsilon = 0.9\n",
    "score = 0\n",
    "comm_flag = 1\n",
    "threshold = -0.1\n",
    "tau = 0.98\n",
    "cost_all = 0\n",
    "cost_comm = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IUxf35c1hpGD"
   },
   "source": [
    "https://github.com/jiechuanjiang/pytorch_DGN/blob/main/Surviving/DGN%2BATOC/buffer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "cXmi6mXkht2b"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class ReplayBuffer(object):\n",
    "\n",
    "\tdef __init__(self, buffer_size, obs_space, n_action, n_ant):\n",
    "\t\tself.buffer_size = buffer_size\n",
    "\t\tself.n_ant = n_ant\n",
    "\t\tself.pointer = 0\n",
    "\t\tself.len = 0\n",
    "\t\tself.actions = np.zeros((self.buffer_size,self.n_ant),dtype = np.int32)\n",
    "\t\tself.rewards = np.zeros((self.buffer_size,n_ant))\n",
    "\t\tself.dones = np.zeros((self.buffer_size,1))\n",
    "\t\tself.obs = np.zeros((self.buffer_size,self.n_ant,obs_space))\n",
    "\t\tself.next_obs = np.zeros((self.buffer_size,self.n_ant,obs_space))\n",
    "\t\tself.matrix = np.zeros((self.buffer_size,self.n_ant,self.n_ant))\n",
    "\t\tself.next_matrix = np.zeros((self.buffer_size,self.n_ant,self.n_ant))\n",
    "\n",
    "\tdef getBatch(self, batch_size):\n",
    "\n",
    "\t\tindex = np.random.choice(self.len, batch_size, replace=False)\n",
    "\t\treturn self.obs[index], self.actions[index], self.rewards[index], self.next_obs[index], self.matrix[index], self.next_matrix[index], self.dones[index]\n",
    "\n",
    "\tdef add(self, obs, action, reward, next_obs, matrix, next_matrix, done):\n",
    "\n",
    "\t\tself.obs[self.pointer] = obs\n",
    "\t\tself.actions[self.pointer] = action\n",
    "\t\tself.rewards[self.pointer] = reward\n",
    "\t\tself.next_obs[self.pointer] = next_obs\n",
    "\t\tself.matrix[self.pointer] = matrix\n",
    "\t\tself.next_matrix[self.pointer] = next_matrix\n",
    "\t\tself.dones[self.pointer] = done\n",
    "\t\tself.pointer = (self.pointer + 1)%self.buffer_size\n",
    "\t\tself.len = min(self.len + 1, self.buffer_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wONM8xyshxjL"
   },
   "source": [
    "https://github.com/jiechuanjiang/pytorch_DGN/blob/main/Surviving/DGN%2BATOC/model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "RbOXp3Yph2mM"
   },
   "outputs": [],
   "source": [
    "import math, random\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd \n",
    "import torch.nn.functional as F\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "Variable = lambda *args, **kwargs: autograd.Variable(*args, **kwargs).cuda() if USE_CUDA else autograd.Variable(*args, **kwargs)\n",
    "\n",
    "class ATT(nn.Module):\n",
    "\tdef __init__(self, din):\n",
    "\t\tsuper(ATT, self).__init__()\n",
    "\t\tself.fc1 = nn.Linear(din, 64)\n",
    "\t\tself.fc2 = nn.Linear(64, 64)\n",
    "\t\tself.fc3 = nn.Linear(64, 1)\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\ty = F.relu(self.fc1(x))\n",
    "\t\ty = F.relu(self.fc2(y))\n",
    "\t\ty = F.sigmoid(self.fc3(y))\n",
    "\t\treturn y\n",
    "\t\t\n",
    "class Encoder(nn.Module): # TODO: Need to make it a CNN for higher dim obs space like MetaWorld\n",
    "\tdef __init__(self, din=32, hidden_dim=128):\n",
    "\t\tsuper(Encoder, self).__init__()\n",
    "\t\tself.fc = nn.Linear(din, hidden_dim)\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\tembedding = F.relu(self.fc(x))\n",
    "\t\treturn embedding\n",
    "\n",
    "class AttModel(nn.Module):\n",
    "\tdef __init__(self, n_node, din, hidden_dim, dout):\n",
    "\t\tsuper(AttModel, self).__init__()\n",
    "\t\tself.fcv = nn.Linear(din, hidden_dim)\n",
    "\t\tself.fck = nn.Linear(din, hidden_dim)\n",
    "\t\tself.fcq = nn.Linear(din, hidden_dim)\n",
    "\t\tself.fcout = nn.Linear(hidden_dim, dout)\n",
    "\n",
    "\tdef forward(self, x, mask):\n",
    "\t\tv = F.relu(self.fcv(x))\n",
    "\t\tq = F.relu(self.fcq(x))\n",
    "\t\tk = F.relu(self.fck(x)).permute(0,2,1)\n",
    "\t\tatt = F.softmax(torch.mul(torch.bmm(q,k), mask) - 9e15*(1 - mask),dim=2)\n",
    "        # Note: Order of applying adj matrix is different than that in paper. Don't get confused!\n",
    "\t\tout = torch.bmm(att,v)\n",
    "\t\t#out = torch.add(out,v)\n",
    "\t\t#out = F.relu(self.fcout(out))\n",
    "\t\treturn out\n",
    "\n",
    "class Q_Net(nn.Module):\n",
    "\tdef __init__(self, hidden_dim, dout):\n",
    "\t\tsuper(Q_Net, self).__init__()\n",
    "\t\tself.fc = nn.Linear(hidden_dim, dout)\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\tq = self.fc(x)\n",
    "\t\treturn q\n",
    "\n",
    "class DGN(nn.Module):\n",
    "\tdef __init__(self,n_agent,num_inputs,hidden_dim,num_actions):\n",
    "\t\tsuper(DGN, self).__init__()\n",
    "\t\t\n",
    "\t\tself.encoder = Encoder(num_inputs,hidden_dim)\n",
    "        # TODO: Try both single encoder and mix of encoder settings\n",
    "        # Will remain same for MTRL\n",
    "\t\tself.att_1 = AttModel(n_agent,hidden_dim,hidden_dim,hidden_dim)\n",
    "\t\tself.att_2 = AttModel(n_agent,hidden_dim,hidden_dim,hidden_dim)\n",
    "\t\tself.q_net = Q_Net(hidden_dim,num_actions)\n",
    "        # Q Net remains same for MTRL\n",
    "\t\t\n",
    "\tdef forward(self, x, mask):\n",
    "\t\th1 = self.encoder(x)\n",
    "\t\th2 = self.att_1(h1, mask)\n",
    "\t\th3 = self.att_2(h2, mask)\n",
    "        # TODO: try concatentation for MTRL\n",
    "\t\tq = self.q_net(h3)\n",
    "        # Note: No concatenation done. Output of last attention head used directly\n",
    "        # Note: 2 attention heads used\n",
    "\t\treturn q "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MxG8-x_piggE"
   },
   "source": [
    "https://github.com/jiechuanjiang/pytorch_DGN/blob/main/Surviving/DGN%2BATOC/surviving.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "P1kLrLRBijXp"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "def is_legal(x,y):\n",
    "\n",
    "\treturn (x>=1)&(x<=30)&(y>=1)&(y<=30)\n",
    "\n",
    "class Surviving(object):\n",
    "\tdef __init__(self, n_agent):\n",
    "\t\tsuper(Surviving, self).__init__()\n",
    "\t\tself.n_agent = n_agent\n",
    "\t\tself.n_action = 5\n",
    "\t\tself.max_food = 10\n",
    "\t\tself.capability = 2*self.n_agent\n",
    "\n",
    "\t\tself.maze = self.build_env()\n",
    "\t\tself.ants = []\n",
    "\t\tfor i in range(self.n_agent):\n",
    "\t\t\tself.ants.append([np.random.randint(0,30)+1,np.random.randint(0,30)+1])\n",
    "\n",
    "\t\tself.foods = []\n",
    "\t\tfor i in range(self.n_agent):\n",
    "\t\t\tself.foods.append(self.max_food)\n",
    "\n",
    "\t\tself.n_resource = 8\n",
    "\t\tself.resource = []\n",
    "\t\tself.resource_pos = []\n",
    "\t\tfor i in range(self.n_resource):\n",
    "\t\t\tself.resource_pos.append([np.random.randint(0,30)+1,np.random.randint(0,30)+1])\n",
    "\t\t\tself.resource.append(np.random.randint(100,120))\n",
    "\t\t\n",
    "\t\tself.steps = 0\n",
    "\t\tself.len_obs = 29\n",
    "\n",
    "\tdef reset(self):\n",
    "\n",
    "\t\tself.maze = self.build_env()\n",
    "\n",
    "\t\tself.ants = []\n",
    "\t\tfor i in range(self.n_agent):\n",
    "\t\t\tself.ants.append([np.random.randint(0,30)+1,np.random.randint(0,30)+1])\n",
    "\n",
    "\t\tself.foods = []\n",
    "\t\tfor i in range(self.n_agent):\n",
    "\t\t\tself.foods.append(self.max_food)\n",
    "\n",
    "\t\tself.resource = []\n",
    "\t\tself.resource_pos = []\n",
    "\t\tfor i in range(self.n_resource):\n",
    "\t\t\tself.resource_pos.append([np.random.randint(0,30)+1,np.random.randint(0,30)+1])\n",
    "\t\t\tself.resource.append(np.random.randint(100,120))\n",
    "\n",
    "\t\treturn self.get_obs(), self.get_adj()\n",
    "\n",
    "\tdef build_env(self):\n",
    "\n",
    "\t\tmaze = np.zeros((32,32))\n",
    "\t\tfor i in range(32):\n",
    "\t\t\tmaze[0][i] = -1\n",
    "\t\t\tmaze[i][0] = -1\n",
    "\t\t\tmaze[31][i] = -1\n",
    "\t\t\tmaze[i][31] = -1\n",
    "\n",
    "\t\treturn maze\n",
    "\n",
    "\tdef get_obs(self):\n",
    "\n",
    "\t\tobs = []\n",
    "\n",
    "\t\tmaze_ant = np.zeros((32,32))\n",
    "\t\tfor index in range(self.n_agent):\n",
    "\t\t\tx = self.ants[index][0]\n",
    "\t\t\ty = self.ants[index][1]\n",
    "\t\t\tmaze_ant[x][y] = 1\n",
    "\n",
    "\t\tfor index in range(self.n_agent):\n",
    "\t\t\th = []\n",
    "\t\t\tx = self.ants[index][0]\n",
    "\t\t\ty = self.ants[index][1]\n",
    "\t\t\tfor i in range(5): # TODO - Understand\n",
    "\t\t\t\th.append(np.mod(x,2))\n",
    "\t\t\t\tx = int(x/2)\n",
    "\t\t\tfor i in range(5): # TODO - Understand\n",
    "\t\t\t\th.append(np.mod(y,2))\n",
    "\t\t\t\ty = int(y/2)\n",
    "\t\t\tx_t = self.ants[index][0]\n",
    "\t\t\ty_t = self.ants[index][1]\n",
    "\t\t\tfor i in range(-1,2):\n",
    "\t\t\t\tfor j in range(-1,2):\n",
    "\t\t\t\t\th.append(self.maze[x_t+i][y_t+j])\n",
    "\n",
    "\t\t\tfor i in range(-1,2):\n",
    "\t\t\t\tfor j in range(-1,2):\n",
    "\t\t\t\t\th.append(maze_ant[x_t+i][y_t+j])\n",
    "\n",
    "\t\t\th.append(self.foods[index])\n",
    "\t\t\tobs.append(h)\n",
    "\n",
    "\t\treturn obs\n",
    "\n",
    "\tdef get_adj(self): # TODO: Change this to use task description encoding\n",
    "\n",
    "\t\tadj = np.zeros((self.n_agent,self.n_agent))\n",
    "\n",
    "\t\tfor index in range(self.n_agent):\n",
    "\t\t\tx = self.ants[index][0]\n",
    "\t\t\ty = self.ants[index][1]\n",
    "\t\t\tfor i in range(index):\n",
    "\t\t\t\tx1 = self.ants[i][0]\n",
    "\t\t\t\ty1 = self.ants[i][1]\n",
    "\t\t\t\tif (np.abs(x-x1)<=3)|(np.abs(y-y1)<=3):\n",
    "\t\t\t\t\tadj[index][i] = 1\n",
    "\t\t\t\t\tadj[i][index] = 1\n",
    "\n",
    "\t\treturn adj \n",
    "\n",
    "\n",
    "\tdef step(self,actions):\n",
    "\n",
    "\t\tfor i in range(self.n_agent):\n",
    "\t\t\tx = self.ants[i][0]\n",
    "\t\t\ty = self.ants[i][1]\n",
    "\t\t\t\n",
    "\t\t\tif actions[i] == 0:\n",
    "\t\t\t\tif self.maze[x-1][y]!= -1:\n",
    "\t\t\t\t\t self.ants[i][0] = x-1\n",
    "\t\t\tif actions[i] == 1:\n",
    "\t\t\t\tif self.maze[x+1][y]!= -1:\n",
    "\t\t\t\t\t self.ants[i][0] = x+1\n",
    "\t\t\tif actions[i] == 2:\n",
    "\t\t\t\tif self.maze[x][y-1]!= -1:\n",
    "\t\t\t\t\t self.ants[i][1] = y-1\n",
    "\t\t\tif actions[i] == 3:\n",
    "\t\t\t\tif self.maze[x][y+1]!= -1:\n",
    "\t\t\t\t\t self.ants[i][1] = y+1\n",
    "\t\t\tif actions[i] == 4:\n",
    "\t\t\t\tself.foods[i] += 2*self.maze[x][y]\n",
    "\t\t\t\tself.maze[x][y] = 0\n",
    "\n",
    "\t\t\tself.foods[i] = max(0,min(self.foods[i]-1,self.max_food))\n",
    "\n",
    "\t\treward = [0.4]*self.n_agent\n",
    "\t\tfor i in range(self.n_agent):\n",
    "\t\t\tif self.foods[i] == 0:\n",
    "\t\t\t\treward[i] = - 0.2\n",
    "\n",
    "\t\tdone = False\n",
    "\n",
    "\t\tif (self.maze.sum()+120) > self.capability: # TODO: Understand\n",
    "\n",
    "\t\t\treturn self.get_obs(), self.get_adj(), reward, done\n",
    "\n",
    "\t\tfor i in range(self.n_resource):\n",
    "\n",
    "\t\t\tx = self.resource_pos[i][0] + np.random.randint(-3,4)\n",
    "\t\t\ty = self.resource_pos[i][1] + np.random.randint(-3,4)\n",
    "\n",
    "\t\t\tif is_legal(x,y):\n",
    "\n",
    "\t\t\t\tnum = np.random.randint(1,6)\n",
    "\t\t\t\tself.maze[x][y] += num\n",
    "\t\t\t\tself.maze[x][y] = min(self.maze[x][y],5)\n",
    "\t\t\t\tself.resource[i] -= num\n",
    "\n",
    "\t\t\t\tif self.resource[i] <= 0:\n",
    "\t\t\t\t\tself.resource_pos[i][0] = np.random.randint(0,30)+1\n",
    "\t\t\t\t\tself.resource_pos[i][1] = np.random.randint(0,30)+1\n",
    "\t\t\t\t\tself.resource[i] = np.random.randint(100,120)\n",
    "\n",
    "\t\treturn self.get_obs(), self.get_adj(), reward, done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0D64gsJRimGp"
   },
   "source": [
    "https://github.com/jiechuanjiang/pytorch_DGN/blob/main/Surviving/DGN%2BATOC/main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 400
    },
    "id": "gNfVZBScitk1",
    "outputId": "1c74fa00-341a-479c-e219-183896503acb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-75.00489999999974\n",
      "-75.53019999999992\n",
      "-77.38300000000028\n",
      "-72.57520000000032\n",
      "-64.32819999999981\n",
      "-60.59439999999998\n",
      "-60.824799999999904\n",
      "-57.964600000000374\n",
      "-55.56669999999988\n",
      "-54.08619999999985\n",
      "-50.014899999999834\n",
      "-54.027699999999896\n",
      "-51.430299999999846\n",
      "-46.261900000000104\n",
      "-48.02709999999987\n",
      "-44.68840000000008\n",
      "-42.70929999999997\n",
      "-37.68070000000014\n",
      "-34.63389999999993\n",
      "-37.09270000000002\n",
      "-40.64140000000004\n",
      "-30.23559999999998\n",
      "-36.66250000000015\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/state/partition1/job-12597466/ipykernel_1375237/1126447004.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     68\u001b[0m                         \u001b[0maction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m                 \u001b[0mnext_obs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_adj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterminated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m                 \u001b[0mbuff\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_obs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_adj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnext_adj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mterminated\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/state/partition1/job-12597466/ipykernel_1375237/2999681431.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    168\u001b[0m                                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresource\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m120\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_obs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_adj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/state/partition1/job-12597466/ipykernel_1375237/2999681431.py\u001b[0m in \u001b[0;36mget_obs\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     82\u001b[0m                                 \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m                         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m                                 \u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m                                 \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m                         \u001b[0mx_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mants\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import math, random, copy\n",
    "import numpy as np\n",
    "import os,sys\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd \n",
    "import torch.nn.functional as F\n",
    "\n",
    "# from model import DGN, ATT\n",
    "# from buffer import ReplayBuffer\n",
    "# from surviving import Surviving\n",
    "# from config import *\n",
    "\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "\n",
    "env = Surviving(n_agent = 100)\n",
    "n_ant = env.n_agent\n",
    "observation_space = env.len_obs\n",
    "n_actions = env.n_action\n",
    "\n",
    "buff = ReplayBuffer(capacity,observation_space,n_actions,n_ant)\n",
    "model = DGN(n_ant,observation_space,hidden_dim,n_actions)\n",
    "model_tar = DGN(n_ant,observation_space,hidden_dim,n_actions)\n",
    "model = model.cuda()\n",
    "model_tar = model_tar.cuda()\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.0001)\n",
    "att = ATT(observation_space).cuda()\n",
    "att_tar = ATT(observation_space).cuda()\n",
    "att_tar.load_state_dict(att.state_dict())\n",
    "optimizer_att = optim.Adam(att.parameters(), lr = 0.0001)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "M_Null = torch.Tensor(np.array([np.eye(n_ant)]*batch_size)).cuda()\n",
    "M_ZERO = torch.Tensor(np.zeros((batch_size,n_ant,n_ant))).cuda()\n",
    "# threshold = float(sys.argv[1]) TODO: figure this out\n",
    "# f = open(sys.argv[1]+'-'+sys.argv[2]+'.txt','w+')\n",
    "f = open(\"TRIAL.txt\", \"w+\")\n",
    "while i_episode<n_episode:\n",
    "\n",
    "\tif i_episode > 40:\n",
    "\t\tepsilon -= 0.001\n",
    "\t\tif epsilon < 0.01:\n",
    "\t\t\tepsilon = 0.01\n",
    "\ti_episode+=1\n",
    "\tsteps = 0\n",
    "\tobs, adj = env.reset()\n",
    "\twhile steps < max_step:\n",
    "\t\tsteps+=1 \n",
    "\t\taction=[]\n",
    "\t\tcost_all += adj.sum()\n",
    "\t\tv_a = np.array(att(torch.Tensor(np.array([obs])).cuda())[0].cpu().data)\n",
    "\t\tfor i in range(n_ant):\n",
    "\t\t\tif np.random.rand() < epsilon:\n",
    "\t\t\t\tadj[i] = adj[i]*0 if np.random.rand() < 0.5 else adj[i]*1\n",
    "\t\t\telse:\n",
    "\t\t\t\tadj[i] = adj[i]*0 if v_a[i][0] < threshold else adj[i]*1\n",
    "        # Note: above loop is epsilon greedy exploration to give less importance to observations that fall below a certain threshold\n",
    "\t\tn_adj = adj*comm_flag\n",
    "\t\tcost_comm += n_adj.sum()\n",
    "\t\tn_adj = n_adj + np.eye(n_ant)\n",
    "\t\tq = model(torch.Tensor(np.array([obs])).cuda(), torch.Tensor(np.array([n_adj])).cuda())[0]\n",
    "\t\tfor i in range(n_ant):\n",
    "\t\t\tif np.random.rand() < epsilon:\n",
    "\t\t\t\ta = np.random.randint(n_actions)\n",
    "\t\t\telse:\n",
    "\t\t\t\ta = q[i].argmax().item()\n",
    "\t\t\taction.append(a)\n",
    "\n",
    "\t\tnext_obs, next_adj, reward, terminated = env.step(action)\n",
    "\n",
    "\t\tbuff.add(np.array(obs),action,reward,np.array(next_obs),n_adj,next_adj,terminated)\n",
    "\t\tobs = next_obs\n",
    "\t\tadj = next_adj\n",
    "\t\tscore += sum(reward)\n",
    "\n",
    "\tif i_episode%20==0:\n",
    "\t\tprint(score/2000)\n",
    "#         print(score/2000)\n",
    "\t\tf.write(str(score/2000)+'\t'+str(cost_comm/cost_all)+'\\n')\n",
    "\t\tf.flush()\n",
    "\t\tscore = 0\n",
    "\t\tcost_comm = 0\n",
    "\t\tcost_all = 0\n",
    "\n",
    "\tif i_episode < 40:\n",
    "\t\tcontinue\n",
    "\n",
    "\tfor e in range(n_epoch):\n",
    "\t\t\n",
    "\t\tO,A,R,Next_O,Matrix,Next_Matrix,D = buff.getBatch(batch_size)\n",
    "\t\tO = torch.Tensor(O).cuda()\n",
    "\t\tMatrix = torch.Tensor(Matrix).cuda()\n",
    "\t\tNext_O = torch.Tensor(Next_O).cuda()\n",
    "\t\tNext_Matrix = torch.Tensor(Next_Matrix).cuda()\n",
    "\n",
    "\t\tlabel = model(Next_O, Next_Matrix+M_Null).max(dim = 2)[0] - model(Next_O, M_Null).max(dim = 2)[0]\n",
    "\t\tlabel = (label - label.mean())/(label.std()+0.000001) + 0.5\n",
    "\t\tlabel = torch.clamp(label, 0, 1).unsqueeze(-1).detach()\n",
    "\t\tloss = criterion(att(Next_O), label)\n",
    "\t\toptimizer_att.zero_grad()\n",
    "\t\tloss.backward()\n",
    "\t\toptimizer_att.step()\n",
    "\n",
    "\t\tV_A_D = att_tar(Next_O).expand(-1,-1,n_ant)\n",
    "\t\tNext_Matrix = torch.where(V_A_D > threshold, Next_Matrix, M_ZERO)\n",
    "\t\tNext_Matrix = Next_Matrix*comm_flag + M_Null\n",
    "\n",
    "\t\tq_values = model(O, Matrix)\n",
    "\t\ttarget_q_values = model_tar(Next_O, Next_Matrix).max(dim = 2)[0]\n",
    "\t\ttarget_q_values = np.array(target_q_values.cpu().data)\n",
    "\t\texpected_q = np.array(q_values.cpu().data)\n",
    "\t\t\n",
    "\t\tfor j in range(batch_size):\n",
    "\t\t\tfor i in range(n_ant):\n",
    "\t\t\t\texpected_q[j][i][A[j][i]] = R[j][i] + (1-D[j])*GAMMA*target_q_values[j][i]\n",
    "\t\t\n",
    "\t\tloss = (q_values - torch.Tensor(expected_q).cuda()).pow(2).mean()\n",
    "\t\toptimizer.zero_grad()\n",
    "\t\tloss.backward()\n",
    "\t\toptimizer.step()\n",
    "\n",
    "\t\twith torch.no_grad():\n",
    "\t\t\tfor p, p_targ in zip(model.parameters(), model_tar.parameters()):\n",
    "\t\t\t\tp_targ.data.mul_(tau)\n",
    "\t\t\t\tp_targ.data.add_((1 - tau) * p.data)\n",
    "\t\t\tfor p, p_targ in zip(att.parameters(), att_tar.parameters()):\n",
    "\t\t\t\tp_targ.data.mul_(tau)\n",
    "\t\t\t\tp_targ.data.add_((1 - tau) * p.data)\n",
    "\t\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EatingInMaze():\n",
    "    # Class for eating some foods in a maze as efficient as possible\n",
    "    # The agents will be out to random places\n",
    "    def __init__(self, num_of_agents, num_of_foods, grid_size, communication_range):\n",
    "        # grid_size: tuple of width and height of the maze\n",
    "        # communication_range: number of cells that a certain agent can communicate \n",
    "        self.num_of_agents = num_of_agents\n",
    "        self.num_of_foods = num_of_foods\n",
    "        self.grid_size = grid_size\n",
    "        # observations will be an array with the maze values\n",
    "        self.obs_range = 1 # number of cells around each agent that an agent can observe\n",
    "        self.comm_range = communication_range # number of cells around each agent that an agent can communicate with another agent - helps with cooperation\n",
    "\n",
    "        self.max_life = 10 # Each agent will have 10 lives in the beginning\n",
    "\n",
    "        self.reset()\n",
    "        \n",
    "    def get_maze_index(self, rand_index):\n",
    "        return (int(rand_index/self.grid_size[1]), int(rand_index%self.grid_size[1]))\n",
    "\n",
    "    def get_agent_index(self, x, y):\n",
    "        return int(x * self.grid_size[1] + y)\n",
    "\n",
    "    def is_inside_maze(self, x, y):\n",
    "        return (x >= 0) & (x < self.grid_size[0]) & (y >= 0) & (y < self.grid_size[1])\n",
    "\n",
    "    def build_maze(self): # The method to build the maze \n",
    "        # TODO\n",
    "        # Put the agents in a random position in the maze \n",
    "        # And add some random foods\n",
    "        self.maze = np.zeros((self.grid_size[0], self.grid_size[1]))\n",
    "        # print(self.maze)\n",
    "        # Randomly put the foods and agents here\n",
    "        self.food_indices = np.ones(self.num_of_foods) * -1\n",
    "        self.agent_indices = np.ones(self.num_of_agents) * -1\n",
    "        for i in range(self.num_of_foods + self.num_of_agents):\n",
    "            rand_index = np.random.randint(0, self.grid_size[0] * self.grid_size[1] - 1)\n",
    "            while (rand_index in self.food_indices) or (rand_index in self.agent_indices): # Make sure that food and agent will not be put in the same cell\n",
    "                rand_index = np.random.randint(0, self.grid_size[0] * self.grid_size[1] - 1)\n",
    "\n",
    "            if i < self.num_of_foods:\n",
    "                self.food_indices[i] = rand_index\n",
    "                # Put the food to the maze - if there is a food in the cell then maze should have 1 in that particular cell\n",
    "                self.maze[self.get_maze_index(rand_index)[0]][self.get_maze_index(rand_index)[1]] = 1\n",
    "            else:\n",
    "                self.agent_indices[i - self.num_of_foods] = rand_index\n",
    "                # Put the agent to the maze - if there is an agent in the cell then the maze should have 2\n",
    "                self.maze[self.get_maze_index(rand_index)[0]][self.get_maze_index(rand_index)[1]] = 2 # TODO: this might be problematic\n",
    "\n",
    "        # print('self.food_indices: {}, self.agent_indices: {}'.format(self.food_indices, self.agent_indices))\n",
    "        print('maze:\\n{}'.format(self.maze))\n",
    "\n",
    "    def reset(self): # Resets the environment\n",
    "        # TODO\n",
    "\n",
    "        self.lives = np.ones(self.num_of_agents) * self.max_food\n",
    "\n",
    "        self.build_maze()\n",
    "        obs = self.get_obs()\n",
    "        adj = self.get_adj()\n",
    "\n",
    "        return obs, adj\n",
    "\n",
    "    def get_obs(self):\n",
    "        # Traverse the whole maze and for each agent get the maze values from the surroundings\n",
    "        obs = []\n",
    "        for agent_index in self.agent_indices:\n",
    "            x, y = self.get_maze_index(agent_index) # Get the position of the agent in the grid\n",
    "            print('agent_index: {}, x: {}, y: {}, get_agent_index(x,y): {}'.format(agent_index, x, y, self.get_agent_index(x,y)))\n",
    "            # Get the values from cells that are self.obs_range away from the agent\n",
    "            curr_obs = []\n",
    "            for i in range(-1,2): # i will be -1,0,1\n",
    "                for j in range(-1,2):\n",
    "                    if i != 0 or j != 0: # We don't want to get the agent exactly\n",
    "                        if self.is_inside_maze(x+i, y+j):\n",
    "                            curr_obs.append(self.maze[x+i][y+j])\n",
    "                        else:\n",
    "                            curr_obs.append(-1) # If this is not good then it will be -1\n",
    "\n",
    "            # print('curr_obs: {}'.format(curr_obs))\n",
    "            obs.append(curr_obs)\n",
    "            \n",
    "        print('obs:\\n{}'.format(np.array(obs)))\n",
    "        return obs\n",
    "\n",
    "    def get_adj(self):\n",
    "        # TODO\n",
    "\n",
    "        adj = np.zeros((self.num_of_agents, self.num_of_agents))\n",
    "\n",
    "        # Check if there are any agents that are closer to her than communication range and insert 1 in the adjacency matrix\n",
    "        # if that is the case\n",
    "        for i in range(self.num_of_agents):\n",
    "            curr_x, curr_y = self.get_maze_index(self.agent_indices[i])\n",
    "            for j in range(self.num_of_agents):\n",
    "                other_x, other_y = self.get_maze_index(self.agent_indices[j])\n",
    "                # Calculate the distance between current and other agents\n",
    "                dist = math.sqrt((curr_x - other_x)**2 + (curr_y - other_y)**2)\n",
    "                if dist < self.comm_range:\n",
    "                    adj[i][j] = 1\n",
    "\n",
    "        print('adj:\\n{}'.format(adj))\n",
    "        return adj\n",
    "\n",
    "    def step(self, actions):\n",
    "        # TODO\n",
    "        # actions: [agent#1 action, agent#2 action, ...]\n",
    "\n",
    "        for i in range(self.num_of_agents):\n",
    "            x, y = self.get_maze_index(self.agent_indices[i])\n",
    "\n",
    "            if actions[i] == 0: # Move up (decrease x by one)\n",
    "                if self.is_inside_maze(x-1,y): # We can go up\n",
    "                    # Change agent_indices and maze\n",
    "                    self.agent_indices[i] = self.get_agent_index(x-1, y)\n",
    "                    self.maze[x][y] = 0\n",
    "                    self.maze[x-1][y] = 2 # Moved the agent\n",
    "            \n",
    "            if actions[i] == 1: # Move down (increase x by one)\n",
    "                if self.is_inside_maze(x+1,y): # We can go up\n",
    "                    # Change agent_indices and maze\n",
    "                    self.agent_indices[i] = self.get_agent_index(x+1, y)\n",
    "                    self.maze[x][y] = 0\n",
    "                    self.maze[x+1][y] = 2 # Moved the agent\n",
    "            \n",
    "            if actions[i] == 2: # Move left (decrease Y by one)\n",
    "                if self.is_inside_maze(x,y-1): # We can go left\n",
    "                    # Change agent_indices and maze\n",
    "                    self.agent_indices[i] = self.get_agent_index(x, y-1)\n",
    "                    self.maze[x][y] = 0\n",
    "                    self.maze[x][y-1] = 2 # Moved the agent\n",
    "            \n",
    "            if actions[i] == 3: # Move left (decrease Y by one)\n",
    "                if self.is_inside_maze(x,y+1): # We can go left\n",
    "                    # Change agent_indices and maze\n",
    "                    self.agent_indices[i] = self.get_agent_index(x, y+1)\n",
    "                    self.maze[x][y] = 0\n",
    "                    self.maze[x][y+1] = 2 # Moved the agent\n",
    "\n",
    "            if actions[i] == 4: # Eat the food in this cell\n",
    "                self.maze[x][y] = 0\n",
    "\n",
    "            # Decrease life of each agent by one - in each step life of an agent is decreased\n",
    "            self.lives[i] = max(0,min(self.lives[i]-1,self.max_life))\n",
    "\n",
    "        # Get rewards\n",
    "        reward = [0.4]*self.num_of_agents\n",
    "        for i in range(self.num_of_agents):\n",
    "            if self.lives[i] == 0: # self.foods also stand for health\n",
    "                reward[i] = - 0.2\n",
    "\n",
    "        # If there are no more foods in the whole maze, then the task is done\n",
    "        done = not (1 in self.maze)\n",
    "        print('done: {}'.format(done)) \n",
    "\n",
    "        return self.get_obs(), self.get_adj(), reward, done\n",
    "\n",
    "\n",
    "env = EatingInMaze(num_of_agents=6, num_of_foods=5, grid_size=(6,5), communication_range=3)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "DRL-FinalProject-GridWorld.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "my_pytorch_env",
   "language": "python",
   "name": "my_pytorch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
