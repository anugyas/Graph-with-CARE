{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iTXhOLj8hfJN"
   },
   "source": [
    "https://github.com/jiechuanjiang/pytorch_DGN/blob/main/Surviving/DGN%2BATOC/config.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/share/apps/python/3.8.6/intel/lib/python3.8/site-packages/setuptools/distutils_patch.py:25: UserWarning: Distutils was imported before Setuptools. This usage is discouraged and may exhibit undesirable behaviors or errors. Please use Setuptools' objects directly or at least import Setuptools first.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(14)\n",
    "import math, random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd \n",
    "import torch.nn.functional as F\n",
    "import copy\n",
    "import os,sys\n",
    "from torch.utils import tensorboard as tb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "eJJIdzHAhTGs"
   },
   "outputs": [],
   "source": [
    "GRID_DIM = 50 # TODO: Tune this\n",
    "NUM_TASKS = 5 # TODO: Tune this\n",
    "NUM_ATTENTION_HEADS = 5\n",
    "ADJ_THRESHOLD = GRID_DIM / 4 # TODO: Tune this\n",
    "WAIT_TILL_ALL_TASKS_DONE = False\n",
    "USE_OBS_DIST = False\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "Variable = lambda *args, **kwargs: autograd.Variable(*args, **kwargs).cuda() if USE_CUDA else autograd.Variable(*args, **kwargs)\n",
    "\n",
    "def is_legal(x,y):\n",
    "    return (x>=0)&(x<GRID_DIM)&(y>=0)&(y<=GRID_DIM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IUxf35c1hpGD"
   },
   "source": [
    "https://github.com/jiechuanjiang/pytorch_DGN/blob/main/Surviving/DGN%2BATOC/buffer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBufferGCare(object):\n",
    "    \"\"\"\n",
    "    Replay buffer for storing the agent's experiences\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, buffer_size, obs_space, n_action, n_tasks):\n",
    "        \"\"\"\n",
    "        Initialize the replay buffer\n",
    "        \n",
    "        Params:\n",
    "        buffer_size:\n",
    "        obs_space:\n",
    "        n_action:\n",
    "        n_tasks:\n",
    "        \"\"\"\n",
    "        self.buffer_size = buffer_size\n",
    "        self.n_tasks = n_tasks\n",
    "        self.pointer = 0\n",
    "        self.len = 0\n",
    "        self.actions = np.zeros((self.buffer_size,1),dtype = np.int32)\n",
    "        self.rewards = np.zeros((self.buffer_size, 1))\n",
    "        self.dones = np.zeros((self.buffer_size,1))\n",
    "        self.obs = np.zeros((self.buffer_size,n_tasks,obs_space))\n",
    "        self.next_obs = np.zeros((self.buffer_size,n_tasks,obs_space))\n",
    "        self.matrix = np.zeros((self.buffer_size,self.n_tasks,self.n_tasks))\n",
    "        self.next_matrix = np.zeros((self.buffer_size,self.n_tasks,self.n_tasks))\n",
    "\n",
    "    def getBatch(self, batch_size):\n",
    "        \"\"\"\n",
    "        Sample a batch of random entries from the replay buffer\n",
    "        \n",
    "        Params:\n",
    "        batch_size:\n",
    "        \n",
    "        Returns:\n",
    "        obs:\n",
    "        action:\n",
    "        reward\n",
    "        next_obs:\n",
    "        matrix:\n",
    "        next_matrix:\n",
    "        done:\n",
    "        \"\"\"\n",
    "        index = np.random.choice(self.len, batch_size, replace=False)\n",
    "        return self.obs[index], self.actions[index], self.rewards[index], self.next_obs[index], self.matrix[index], self.next_matrix[index], self.dones[index]\n",
    "\n",
    "    def add(self, obs, action, reward, next_obs, matrix, next_matrix, done):\n",
    "        \"\"\"\n",
    "        Add to the replay buffer\n",
    "        \n",
    "        Params:\n",
    "        obs:\n",
    "        action:\n",
    "        reward:\n",
    "        next_obs:\n",
    "        matrix:\n",
    "        next_matrix:\n",
    "        done:\n",
    "        \"\"\"\n",
    "        self.obs[self.pointer] = obs\n",
    "        self.actions[self.pointer] = action\n",
    "        self.rewards[self.pointer] = reward\n",
    "        self.next_obs[self.pointer] = next_obs\n",
    "        self.matrix[self.pointer] = matrix\n",
    "        self.next_matrix[self.pointer] = next_matrix\n",
    "        self.dones[self.pointer] = done\n",
    "        self.pointer = (self.pointer + 1)%self.buffer_size\n",
    "        self.len = min(self.len + 1, self.buffer_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wONM8xyshxjL"
   },
   "source": [
    "https://github.com/jiechuanjiang/pytorch_DGN/blob/main/Surviving/DGN%2BATOC/model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MTRL_ATT(nn.Module):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    def __init__(self, din):\n",
    "        super(MTRL_ATT, self).__init__()\n",
    "        self.fc1 = nn.Linear(din, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = F.relu(self.fc1(x))\n",
    "        y = F.relu(self.fc2(y))\n",
    "        y = F.sigmoid(self.fc3(y))\n",
    "        return y\n",
    "\n",
    "class MTRL_Encoder(nn.Module): # TODO: Need to make it a CNN for higher dim obs space like MetaWorld\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    def __init__(self, din=32, hidden_dim=128):\n",
    "        super(MTRL_Encoder, self).__init__()\n",
    "        self.fc = nn.Linear(din, hidden_dim)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedding = F.tanh(self.fc(x))\n",
    "        return embedding\n",
    "\n",
    "class MTRL_AttModel(nn.Module):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    def __init__(self, n_node, din, hidden_dim, dout):\n",
    "        super(MTRL_AttModel, self).__init__()\n",
    "        self.fcv = nn.Linear(din, hidden_dim)\n",
    "        self.fck = nn.Linear(din, hidden_dim)\n",
    "        self.fcq = nn.Linear(din, hidden_dim)\n",
    "        self.fcout = nn.Linear(hidden_dim, dout)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        v = F.tanh(self.fcv(x))\n",
    "        q = F.tanh(self.fcq(x))\n",
    "        k = F.tanh(self.fck(x)).permute(0,2,1)\n",
    "        att = F.softmax(torch.mul(torch.bmm(q,k), mask) - 9e15*(1 - mask),dim=2)\n",
    "        # Note: Order of applying adj matrix is different than that in paper. Don't get confused!\n",
    "        out = torch.bmm(att,v)\n",
    "        return out\n",
    "\n",
    "class MTRL_Q_Net(nn.Module):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_dim, dout):\n",
    "        super(MTRL_Q_Net, self).__init__()\n",
    "        # NOTE: This is now modified to have both h vectors from both of the attention layers\n",
    "        # concatenated - originally it was only getting the h vector of the last layer\n",
    "        # so the input dim of the linear layer was hidden_dim\n",
    "        self.fc = nn.Linear(hidden_dim*(NUM_ATTENTION_HEADS + 1), dout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        q = F.relu(self.fc(x))\n",
    "        return q\n",
    "\n",
    "    \n",
    "class MTRL_DGN(nn.Module):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    def __init__(self,n_tasks,num_inputs,hidden_dim,num_actions):\n",
    "        super(MTRL_DGN, self).__init__()\n",
    "\n",
    "        self.encoder = MTRL_Encoder(num_inputs,hidden_dim)\n",
    "        self.attention_heads = [MTRL_AttModel(n_tasks,hidden_dim,hidden_dim,hidden_dim).cuda() for _ in range(NUM_ATTENTION_HEADS)]\n",
    "        self.q_net = MTRL_Q_Net(hidden_dim,num_actions)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        mask = mask.cuda()\n",
    "        h = self.encoder(x)\n",
    "        attention_heads = [h]\n",
    "        for i in range(NUM_ATTENTION_HEADS):\n",
    "            h = h.cuda()\n",
    "            h = self.attention_heads[i](h, mask)\n",
    "            attention_heads.append(h)\n",
    "#         h2 = self.att_1(h1, mask)\n",
    "#         h3 = self.att_2(h2, mask) \n",
    "        \n",
    "        # TODO: try concatentation for MTRL\n",
    "        \n",
    "        h = torch.cat(attention_heads, dim=-1)\n",
    "#         h4 = torch.cat((h1,h2,h3),dim=-1)\n",
    "        q = self.q_net(h)\n",
    "        # Note: No concatenation done. Output of last attention head used directly\n",
    "        # Note: 2 attention heads used\n",
    "        return q "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MxG8-x_piggE"
   },
   "source": [
    "https://github.com/jiechuanjiang/pytorch_DGN/blob/main/Surviving/DGN%2BATOC/surviving.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorldWithCare(object):\n",
    "    \n",
    "    def __init__(self, n_tasks):\n",
    "        \"\"\"\n",
    "        Initialize the gridworld\n",
    "        \n",
    "        Params:\n",
    "        n_tasks:\n",
    "        \"\"\"\n",
    "        super(GridWorldWithCare, self).__init__()\n",
    "        self.n_action = 4\n",
    "        self.n_tasks = n_tasks\n",
    "        # TODO: maybe include food as part of task, reach dest with > 0 food or something\n",
    "        self.tasks = [0]*self.n_tasks\n",
    "        self.agent = [-1, -1]\n",
    "        self.build_env()\n",
    "\n",
    "        self.dones = np.zeros(self.n_tasks) # Array to indicate whether each task is done or not -- used to calculate rewards\n",
    "        self.steps = 0\n",
    "        if USE_OBS_DIST:\n",
    "            self.len_obs = self.n_tasks + 2\n",
    "        else:\n",
    "            self.len_obs = (self.n_tasks+1)*2\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Reset the gridworld\n",
    "        \n",
    "        Returns:\n",
    "        obs:\n",
    "        adj:\n",
    "        \"\"\"\n",
    "\n",
    "        self.build_env()\n",
    "        self.dones = np.zeros(self.n_tasks)\n",
    "        self.steps = 0\n",
    "        return self.get_obs(), self.get_adj()\n",
    "\n",
    "    def build_env(self):\n",
    "        \"\"\"\n",
    "        Build the gridworld\n",
    "        \"\"\"\n",
    "        for i in range(self.n_tasks):\n",
    "            x = np.random.randint(0, GRID_DIM)\n",
    "            y = np.random.randint(0, GRID_DIM)\n",
    "            self.tasks[i] = [x, y]\n",
    "            print(\"TASK NUMBER \", i, \" DEST: \", x, y)\n",
    "        self.agent[0] = np.random.randint(0, GRID_DIM)\n",
    "        self.agent[1] = np.random.randint(0, GRID_DIM)\n",
    "\n",
    "    def get_obs(self):\n",
    "        \"\"\"\n",
    "        Get observations\n",
    "        \n",
    "        Returns:\n",
    "        obs:\n",
    "        \"\"\"\n",
    "        # TODO: change this for MTRL \n",
    "        obs = []\n",
    "        \n",
    "        x_agent = self.agent[0]\n",
    "        y_agent = self.agent[1]\n",
    "\n",
    "        obs.append(x_agent/GRID_DIM)\n",
    "        obs.append(y_agent/GRID_DIM)\n",
    "\n",
    "        # \t\tfor i in range(-1,2):\n",
    "        # \t\t\tfor j in range(-1,2):\n",
    "        # \t\t\t\tobs.append(self.maze[x_agent+i][y_agent+j])\n",
    "\n",
    "        if USE_OBS_DIST:\n",
    "            for i in range(self.n_tasks):\n",
    "                obs.append(math.sqrt((self.tasks[i][0]-x_agent)**2 + (self.tasks[i][1]-y_agent)**2)/GRID_DIM)\n",
    "        else:\n",
    "            for i in range(self.n_tasks):\n",
    "                obs.append((self.tasks[i][0]-x_agent)/GRID_DIM)\n",
    "                obs.append((self.tasks[i][1]-y_agent)/GRID_DIM)\n",
    "\n",
    "        # TODO: 1. if we include maze state or not, and if we do, we would need to figure out\n",
    "        # how to effectively send that along with task destinations\n",
    "        \n",
    "        #Idea: use distance between agent and task as obs\n",
    "        \n",
    "        return obs\n",
    "\n",
    "    def get_adj(self): # TODO: Change this to use task description encoding. \n",
    "        # In this case task description is the location of the destination.\n",
    "        \"\"\"\n",
    "        Get adjacency matrix\n",
    "        \n",
    "        Returns:\n",
    "        adj:\n",
    "        \"\"\"\n",
    "        adj = np.zeros((self.n_tasks, self.n_tasks))\n",
    "\n",
    "        # Calculate adjacency regarding to the distances of the tasks respect to the agent\n",
    "        x_agent, y_agent = self.agent[0], self.agent[1]\n",
    "\n",
    "        # HARD ATTENTION\n",
    "        # Traverse through the tasks and calculate the Euclidean distance between them and the agent\n",
    "#         for i in range(self.n_tasks):\n",
    "#             x_task_i, y_task_i = self.tasks[i][0] - x_agent, self.tasks[i][1] - y_agent\n",
    "#             for j in range(self.n_tasks):\n",
    "#                 x_task_j, y_task_j = self.tasks[j][0] - x_agent, self.tasks[j][1] - y_agent\n",
    "#                 task_dist = math.sqrt((x_task_j - x_task_i)**2 + (y_task_i - y_task_j)**2)\n",
    "#                 if task_dist <= ADJ_THRESHOLD:\n",
    "#                     adj[i,j] = 1\n",
    "#                     adj[j,i] = 1\n",
    "                    \n",
    "        # SOFT ATTENTION\n",
    "#         adj = np.ones((self.n_tasks, self.n_tasks)) # NOTE: \n",
    "        for i in range(self.n_tasks):\n",
    "            x_task_i, y_task_i = self.tasks[i][0]-x_agent, self.tasks[i][1]-y_agent\n",
    "            for j in range(self.n_tasks):\n",
    "                x_task_j, y_task_j = self.tasks[j][0]-x_agent, self.tasks[j][1]-y_agent\n",
    "                # Instead of having 1 or 0s, have their vectoral positions according to each other\n",
    "                task_dist = math.sqrt((x_task_j - x_task_i)**2 + (y_task_j - y_task_i)**2)\n",
    "                \n",
    "                # Set this distance / GRID_DIM\n",
    "                adj[i,j] = 1 - float(task_dist)/GRID_DIM # Extract from 1 bc the closer the better\n",
    "                adj[j,i] = 1 - float(task_dist)/GRID_DIM\n",
    "\n",
    "        return adj\n",
    "\n",
    "\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Take one step in the gridworld according to the given actions\n",
    "        \n",
    "        Params:\n",
    "        action:\n",
    "        \n",
    "        Returns:\n",
    "        obs:\n",
    "        adj:\n",
    "        reward:\n",
    "        all_tasks_done:\n",
    "        \"\"\"\n",
    "\n",
    "        # There are 4 different actions for the agent\n",
    "        # If there is any place to go in the maze then the agent will go \n",
    "        # 0: Move up, 1: Move down, 2: Move left, 3: Move right\n",
    "\n",
    "        self.steps += 1\n",
    "        x_agent, y_agent = self.agent[0], self.agent[1]\n",
    "        if action == 0: # Move up (decrease x by one)\n",
    "            if is_legal(x_agent-1, y_agent):\n",
    "                # Change the agent and the maze\n",
    "                self.agent[0] -= 1\n",
    "\n",
    "        elif action == 1: # Move down (increase x by one)\n",
    "            if is_legal(x_agent+1, y_agent):\n",
    "                # Change the agent and the maze\n",
    "                self.agent[0] += 1\n",
    "\n",
    "        elif action == 2: # Move left (decrease y by one)\n",
    "            if is_legal(x_agent, y_agent-1):\n",
    "                # Change the agent and the maze\n",
    "                self.agent[1] -= 1\n",
    "\n",
    "        elif action == 3: # Move right (increase y by one)\n",
    "            if is_legal(x_agent, y_agent+1):\n",
    "                # Change the agent and the maze\n",
    "                self.agent[1] += 1\n",
    "                \n",
    "        # Calculate the rewards for each task\n",
    "        rewards = [0] * self.n_tasks\n",
    "        total_reward = 0\n",
    "\n",
    "        # Check if you reached to any destinations here\n",
    "        new_agent_x, new_agent_y = self.agent[0], self.agent[1]\n",
    "        for i in range(self.n_tasks):\n",
    "            if self.tasks[i][0] == new_agent_x and self.tasks[i][1] == new_agent_y:\n",
    "                if self.dones[i] == 0:\n",
    "                    self.dones[i] = 1\n",
    "                    rewards[i] = 10\n",
    "                    total_reward += 10\n",
    "                    print(\"Task \", i, \" completed at step \", self.steps)\n",
    "        # TODO: Uncomment these lines for soft reward\n",
    "#             else:\n",
    "#                 total_reward += 1.0/float((math.sqrt((self.tasks[i][0]-new_agent_x)**2 + (self.tasks[i][1]-new_agent_y)**2)))\n",
    "                \n",
    "\n",
    "        # Only if all the tasks are done, then the episode is done\n",
    "        all_tasks_done = not (0 in self.dones)\n",
    "        score = sum(self.dones) / len(self.dones)\n",
    "        \n",
    "\n",
    "\n",
    "        return self.get_obs(), self.get_adj(), total_reward, all_tasks_done, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class for making the training with different set of hyper parameters\n",
    "class Training:\n",
    "    \"\"\"\n",
    "    This class trains the model and holds the highest scores and the hyper parameter combination that gave us that\n",
    "    highest score. \n",
    "    Prints tryout_index -> highest_score_in_that_tryout for each tryout to the file\n",
    "    \n",
    "    params:\n",
    "    test_results_file: name of the file to hold the results\n",
    "    vizier: boolean to indicate whether to do hyper parameter search\n",
    "    \"\"\"\n",
    "    def __init__(self, file_name, vizier=False): # vizier indicates whether to do hyper param research\n",
    "        self.file_name = file_name\n",
    "        self.file = open(file_name, \"w+\")\n",
    "        self.vizier = vizier # if vizier is true then the class makes \n",
    "\n",
    "        if self.vizier:\n",
    "\n",
    "            # Dictionary to give us all options for hyper parameters\n",
    "            self.hyper_params = {\n",
    "                'hidden_dim': [64], # 2\n",
    "                'max_step': [5000], # 1\n",
    "                'gamma': [0.99], # 1\n",
    "                'n_episode': [800], # 1\n",
    "                'buffer_size': [100000, 10000000], # 3\n",
    "                'batch_size': [128], # 1\n",
    "                'n_epoch': [100], # 2\n",
    "                'epsilon': [0.5, 0.7, 0.9], # 3\n",
    "                'tau': [0.95], # 2\n",
    "                'learning_rate': [0.0005, 0.001, 0.005] # 3\n",
    "            }\n",
    "\n",
    "            self.tryouts = []\n",
    "            for key, value in self.hyper_params.items():\n",
    "                tryouts_len = len(self.tryouts)\n",
    "                if tryouts_len == 0:\n",
    "                    for param in value:\n",
    "                        self.tryouts.append({key : param})\n",
    "\n",
    "                else:\n",
    "                    params_len = len(value)\n",
    "                    for i in range(params_len-1):\n",
    "                        for j in range(tryouts_len):\n",
    "                            self.tryouts.append(copy.deepcopy(self.tryouts[j]))\n",
    "\n",
    "                    for j in range(params_len):\n",
    "                        for i in range(tryouts_len):\n",
    "                            self.tryouts[j*tryouts_len+i][key] = value[j]\n",
    "\n",
    "            print('len(tryouts): {}'.format(len(self.tryouts)))\n",
    "            print('tryouts: {}'.format(tryouts))\n",
    "            self.num_tryout = len(self.tryouts)\n",
    "            self.highest_scores = [0] * self.num_tryout # This will hold the highest score in each try out\n",
    "            \n",
    "        else: \n",
    "            \n",
    "            self.hidden_dim = 64\n",
    "            self.max_step = 5000 #originally 500\n",
    "            self.gamma = 0.99\n",
    "            self.n_episode = 800 #originally 800\n",
    "            self.buffer_size = 1000000 #change back to 65000\n",
    "            self.batch_size = 64 #change back to 64\n",
    "            self.n_epoch = 100 #orginally 25\n",
    "            self.epsilon = 0.7 #originally 0.9\n",
    "            self.tau = 0.95\n",
    "            self.learning_rate = 0.00005\n",
    "            \n",
    "            self.num_tryout = 1\n",
    "            self.highest_scores = [0] * self.num_tryout # This will hold the highest score in each try out\n",
    "        \n",
    "            \n",
    "    def train(self):\n",
    "        env = GridWorldWithCare(NUM_TASKS)\n",
    "        observation_space = env.len_obs\n",
    "        n_actions = env.n_action\n",
    "        n_tasks = env.n_tasks\n",
    "        \n",
    "        # Set the hyper parameters \n",
    "        for tryout_index in range(self.num_tryout):\n",
    "            if self.vizier:\n",
    "                self.hidden_dim = self.tryouts[tryout_index]['hidden_dim']\n",
    "                self.max_step = self.tryouts[tryout_index]['max_step']\n",
    "                self.gamma = self.tryouts[tryout_index]['gamma']\n",
    "                self.n_episode = self.tryouts[tryout_index]['n_episode']\n",
    "                self.buffer_size = self.tryouts[tryout_index]['buffer_size']\n",
    "                self.batch_size = self.tryouts[tryout_index]['batch_size']\n",
    "                self.n_epoch = self.tryouts[tryout_index]['n_epoch']\n",
    "                self.epsilon = self.tryouts[tryout_index]['epsilon']\n",
    "                self.tau = self.tryouts[tryout_index]['tau']\n",
    "                self.learning_rate = self.tryouts[tryout_index]['learning_rate']\n",
    "                \n",
    "            print('-------\\nTRYOUT[{}]: (hidden_dim={}, max_step={}, gamma={}, n_episode={}, buffer_size={}, batch_size={}, n_epoch={}, epsilon={}, tau={}, learning_rate={})'.format(\n",
    "                tryout_index, self.hidden_dim, self.max_step, self.gamma, self.n_episode, self.buffer_size, self.batch_size, self.n_epoch, self.epsilon, self.tau, self.learning_rate\n",
    "            ))\n",
    "                \n",
    "                \n",
    "            buff = ReplayBufferGCare(self.buffer_size, observation_space, n_actions, n_tasks)\n",
    "            model = MTRL_DGN(n_tasks, observation_space, self.hidden_dim, n_actions).cuda()\n",
    "            model_tar = MTRL_DGN(n_tasks, observation_space, self.hidden_dim, n_actions).cuda()\n",
    "            optimizer = optim.Adam(model.parameters(), lr = self.learning_rate)\n",
    "            criterion = nn.BCELoss()\n",
    "            \n",
    "            M_Null = torch.Tensor(np.array([np.eye(n_tasks)] * self.batch_size)).cuda()\n",
    "            M_ZERO = torch.Tensor(np.zeros((self.batch_size, n_tasks, n_tasks))).cuda()\n",
    "        \n",
    "            i_episode = 0\n",
    "            score = 0\n",
    "            \n",
    "            if not self.vizier:\n",
    "                tb_summary_writer = tb.SummaryWriter(log_dir = \"./TB-Logs/\"+self.file_name.split(\".txt\")[0])\n",
    "                global_step_count = 0\n",
    "            \n",
    "            while i_episode < self.n_episode:\n",
    "                if i_episode > 40:\n",
    "                    self.epsilon -= 0.001\n",
    "                    if self.epsilon < 0.01:\n",
    "                        self.epsilon = 0.01\n",
    "                        \n",
    "                i_episode+=1\n",
    "                steps = 0\n",
    "                obs, adj = env.reset()\n",
    "                terminated = False\n",
    "                obs = np.resize(obs, (n_tasks, observation_space))\n",
    "                \n",
    "\n",
    "                if WAIT_TILL_ALL_TASKS_DONE:\n",
    "                    while not terminated:\n",
    "                        steps+=1 \n",
    "                        global_step_count += 1\n",
    "                        # Get the action with forward prop and add the obs, adjs to replay buffer\n",
    "                        q = model(torch.Tensor(np.array([obs])).cuda(), torch.Tensor(np.array([adj])).cuda())[0,0,:]\n",
    "                        if np.random.rand() < self.epsilon:\n",
    "                            action = np.random.randint(n_actions)\n",
    "                        else:\n",
    "                            action = q.argmax().item()\n",
    "                        next_obs, next_adj, reward, terminated, step_score = env.step(action)\n",
    "                        if not self.vizier:\n",
    "                            tb_summary_writer.add_scalar(\"Reward on one step\", reward, global_step_count)\n",
    "                        next_obs = np.resize(next_obs, (n_tasks, observation_space))\n",
    "                        buff.add(np.array(obs),action,reward,np.array(next_obs),adj,next_adj,terminated)\n",
    "\n",
    "                        obs = next_obs\n",
    "                        adj = next_adj\n",
    "                        score += step_score\n",
    "                        \n",
    "                else:\n",
    "                    while steps < self.max_step:\n",
    "                        steps+=1 \n",
    "                        global_step_count += 1\n",
    "                        # Get the action with forward prop and add the obs, adjs to replay buffer\n",
    "                        q = model(torch.Tensor(np.array([obs])).cuda(), torch.Tensor(np.array([adj])).cuda())[0,0,:]\n",
    "                        if np.random.rand() < self.epsilon:\n",
    "                            action = np.random.randint(n_actions)\n",
    "                        else:\n",
    "                            action = q.argmax().item()\n",
    "                        next_obs, next_adj, reward, terminated, step_score = env.step(action)\n",
    "                        if not self.vizier:\n",
    "                            tb_summary_writer.add_scalar(\"Reward on one step\", reward, global_step_count)\n",
    "                        next_obs = np.resize(next_obs, (n_tasks, observation_space))\n",
    "                        buff.add(np.array(obs),action,reward,np.array(next_obs),adj,next_adj,terminated)\n",
    "\n",
    "                        obs = next_obs\n",
    "                        adj = next_adj\n",
    "                        score += step_score\n",
    "\n",
    "                if i_episode%20==0:\n",
    "                    print(score/20)\n",
    "                    if not self.vizier:\n",
    "                        tb_summary_writer.add_scalar(\"Score\", score/20, global_step_count)\n",
    "                    else:\n",
    "                        if score/20 > self.highest_scores[tryout_index]:\n",
    "                            self.highest_scores[tryout_index] = score/20\n",
    "                    score = 0\n",
    "                \n",
    "                \n",
    "                episode_loss = 0\n",
    "                \n",
    "                # Train the model\n",
    "                for e in range(self.n_epoch):\n",
    "                    O,A,R,Next_O,Matrix,Next_Matrix,D = buff.getBatch(self.batch_size)\n",
    "                    O = torch.Tensor(O).cuda()\n",
    "                    Matrix = torch.Tensor(Matrix).cuda()\n",
    "                    Next_O = torch.Tensor(Next_O).cuda()\n",
    "                    Next_Matrix = torch.Tensor(Next_Matrix).cuda()\n",
    "\n",
    "                    q_values = model(O, Matrix)\n",
    "                    q_values = model(O, Matrix)[:,0, :]\n",
    "                    target_q_values = model_tar(Next_O, Next_Matrix).max(dim = 2)[0][:,0]\n",
    "                    target_q_values = np.array(target_q_values.cpu().data)\n",
    "                    expected_q = np.array(q_values.cpu().data)\n",
    "\n",
    "                    for j in range(self.batch_size):\n",
    "                        expected_q[j][A[j][0]] = R[j][0] + (1-D[j][0])*self.gamma*target_q_values[j]\n",
    "\n",
    "                    loss = (q_values - torch.Tensor(expected_q).cuda()).pow(2).mean()\n",
    "                    episode_loss += loss\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                    if e%10 == 0:\n",
    "                        with torch.no_grad():\n",
    "                            for p, p_targ in zip(model.parameters(), model_tar.parameters()):\n",
    "                                p_targ.data.mul_(self.tau)\n",
    "                                p_targ.data.add_((1 - self.tau) * p.data)\n",
    "                \n",
    "                if not self.vizier:\n",
    "                    tb_summary_writer.add_scalar('Loss', episode_loss, global_step_count)\n",
    "    \n",
    "            # Print the highest score to the file\n",
    "            self.file.write('Tryout[{}]: (hidden_dim={}, max_step={}, gamma={}, n_episode={}, buffer_size={}, batch_size={}, n_epoch={}, epsilon={}, tau={}, learning_rate={}): \\t ---> \\t Highest Score: {}\\n'.format(\n",
    "                tryout_index, self.hidden_dim, self.max_step, self.gamma, self.n_episode, self.buffer_size, self.batch_size, self.n_epoch, self.epsilon, self.tau, self.learning_rate, self.highest_scores[tryout_index]\n",
    "            ))\n",
    "            \n",
    "            self.file.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TASK NUMBER  0  DEST:  43 24\n",
      "TASK NUMBER  1  DEST:  12 22\n",
      "TASK NUMBER  2  DEST:  7 38\n",
      "TASK NUMBER  3  DEST:  42 28\n",
      "TASK NUMBER  4  DEST:  10 16\n",
      "-------\n",
      "TRYOUT[0]: (hidden_dim=64, max_step=5000, gamma=0.99, n_episode=800, buffer_size=1000000, batch_size=64, n_epoch=100, epsilon=0.7, tau=0.95, learning_rate=0.0005)\n",
      "TASK NUMBER  0  DEST:  26 25\n",
      "TASK NUMBER  1  DEST:  23 43\n",
      "TASK NUMBER  2  DEST:  32 4\n",
      "TASK NUMBER  3  DEST:  13 10\n",
      "TASK NUMBER  4  DEST:  10 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ig2283/.local/lib/python3.8/site-packages/torch/nn/functional.py:1795: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task  4  completed at step  177\n",
      "TASK NUMBER  0  DEST:  17 38\n",
      "TASK NUMBER  1  DEST:  44 33\n",
      "TASK NUMBER  2  DEST:  29 27\n",
      "TASK NUMBER  3  DEST:  11 1\n",
      "TASK NUMBER  4  DEST:  34 39\n",
      "Task  3  completed at step  84\n",
      "TASK NUMBER  0  DEST:  3 25\n",
      "TASK NUMBER  1  DEST:  46 49\n",
      "TASK NUMBER  2  DEST:  14 17\n",
      "TASK NUMBER  3  DEST:  32 36\n",
      "TASK NUMBER  4  DEST:  17 27\n",
      "TASK NUMBER  0  DEST:  24 43\n",
      "TASK NUMBER  1  DEST:  12 45\n",
      "TASK NUMBER  2  DEST:  6 45\n",
      "TASK NUMBER  3  DEST:  22 16\n",
      "TASK NUMBER  4  DEST:  10 33\n",
      "Task  3  completed at step  169\n",
      "TASK NUMBER  0  DEST:  41 16\n",
      "TASK NUMBER  1  DEST:  49 38\n",
      "TASK NUMBER  2  DEST:  0 39\n",
      "TASK NUMBER  3  DEST:  20 0\n",
      "TASK NUMBER  4  DEST:  15 45\n",
      "TASK NUMBER  0  DEST:  11 15\n",
      "TASK NUMBER  1  DEST:  1 15\n",
      "TASK NUMBER  2  DEST:  26 40\n",
      "TASK NUMBER  3  DEST:  13 19\n",
      "TASK NUMBER  4  DEST:  18 30\n",
      "TASK NUMBER  0  DEST:  21 36\n",
      "TASK NUMBER  1  DEST:  6 32\n",
      "TASK NUMBER  2  DEST:  6 6\n",
      "TASK NUMBER  3  DEST:  44 7\n",
      "TASK NUMBER  4  DEST:  26 6\n",
      "TASK NUMBER  0  DEST:  16 33\n",
      "TASK NUMBER  1  DEST:  5 12\n",
      "TASK NUMBER  2  DEST:  18 41\n",
      "TASK NUMBER  3  DEST:  27 9\n",
      "TASK NUMBER  4  DEST:  12 10\n",
      "TASK NUMBER  0  DEST:  44 17\n",
      "TASK NUMBER  1  DEST:  31 26\n",
      "TASK NUMBER  2  DEST:  16 43\n",
      "TASK NUMBER  3  DEST:  48 36\n",
      "TASK NUMBER  4  DEST:  23 30\n",
      "TASK NUMBER  0  DEST:  43 32\n",
      "TASK NUMBER  1  DEST:  30 10\n",
      "TASK NUMBER  2  DEST:  41 32\n",
      "TASK NUMBER  3  DEST:  12 21\n",
      "TASK NUMBER  4  DEST:  45 19\n",
      "Task  4  completed at step  1125\n"
     ]
    }
   ],
   "source": [
    "log_file_name = str(GRID_DIM) + \"DIM_\"+ str(NUM_TASKS) +\"TASKS_\" + str(NUM_ATTENTION_HEADS) + \"ATT_\" + str(int(WAIT_TILL_ALL_TASKS_DONE)) + \"WAITALLTASKS_\" + str(int(USE_OBS_DIST)) + \"USEOBSDIST.txt\"\n",
    "training = Training(log_file_name, False)\n",
    "training.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0D64gsJRimGp"
   },
   "source": [
    "https://github.com/jiechuanjiang/pytorch_DGN/blob/main/Surviving/DGN%2BATOC/main.py"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "DRL-FinalProject-GridWorld.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
